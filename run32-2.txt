{'gpu': '0', 'target_bits': 1.5, 'model_name': 'meta-llama/Llama-2-7b-hf', 'model_path': '/mnt/disk2-part1/pretrained-models-pytorch/llm/llama2_7b', 'data_path': '/mnt/disk2-part1/datasets/llm', 'train_dataset': 'redpajamas', 'seqlen': 4096, 'only_full_ft': False, 'eval': False, 'one_cycle_lr': False, 'n_calib_data': 256, 'n_epochs': 4, 'norm_order': 2.0, 'n_grad_accumu': 8, 'lr': 3e-05, 'weight_decay': 0.0001, 'train_scaling_vectors': False, 'lr_s': 0.0001, 'epoch_s': 2, 'admm_reg': 0.03, 'alter_opt_outer_iters': 260, 'alter_opt_inner_iters': 3, 'save_dir': '/projects/p487-24-1/dbf_rep', 'is_save_ckpt': True}
ms 4096
total params:  6738415616 
linear layers params:  6476267520
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
eval_loader torch.Size([1, 341469])
Collecting i_norm and o_norm for linear layers...
0 tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)
1 tensor(1.9663, device='cuda:0', grad_fn=<NllLossBackward0>)
2 tensor(1.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
3 tensor(1.8805, device='cuda:0', grad_fn=<NllLossBackward0>)
4 tensor(1.6289, device='cuda:0', grad_fn=<NllLossBackward0>)
5 tensor(2.0616, device='cuda:0', grad_fn=<NllLossBackward0>)
6 tensor(1.9931, device='cuda:0', grad_fn=<NllLossBackward0>)
7 tensor(1.2414, device='cuda:0', grad_fn=<NllLossBackward0>)
8 tensor(2.4268, device='cuda:0', grad_fn=<NllLossBackward0>)
9 tensor(1.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
10 tensor(1.9484, device='cuda:0', grad_fn=<NllLossBackward0>)
11 tensor(2.3143, device='cuda:0', grad_fn=<NllLossBackward0>)
12 tensor(1.3872, device='cuda:0', grad_fn=<NllLossBackward0>)
13 tensor(1.9846, device='cuda:0', grad_fn=<NllLossBackward0>)
14 tensor(2.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
15 tensor(1.7729, device='cuda:0', grad_fn=<NllLossBackward0>)
16 tensor(1.8591, device='cuda:0', grad_fn=<NllLossBackward0>)
17 tensor(2.0349, device='cuda:0', grad_fn=<NllLossBackward0>)
18 tensor(1.6280, device='cuda:0', grad_fn=<NllLossBackward0>)
19 tensor(2.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
20 tensor(1.3324, device='cuda:0', grad_fn=<NllLossBackward0>)
21 tensor(1.7564, device='cuda:0', grad_fn=<NllLossBackward0>)
22 tensor(2.2621, device='cuda:0', grad_fn=<NllLossBackward0>)
23 tensor(1.8752, device='cuda:0', grad_fn=<NllLossBackward0>)
24 tensor(1.8930, device='cuda:0', grad_fn=<NllLossBackward0>)
25 tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
26 tensor(2.2891, device='cuda:0', grad_fn=<NllLossBackward0>)
27 tensor(1.8946, device='cuda:0', grad_fn=<NllLossBackward0>)
28 tensor(1.9507, device='cuda:0', grad_fn=<NllLossBackward0>)
29 tensor(2.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
30 tensor(1.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
31 tensor(2.2025, device='cuda:0', grad_fn=<NllLossBackward0>)
32 tensor(1.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
33 tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
34 tensor(0.9738, device='cuda:0', grad_fn=<NllLossBackward0>)
35 tensor(2.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
36 tensor(1.8112, device='cuda:0', grad_fn=<NllLossBackward0>)
37 tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward0>)
38 tensor(1.7647, device='cuda:0', grad_fn=<NllLossBackward0>)
39 tensor(1.8530, device='cuda:0', grad_fn=<NllLossBackward0>)
40 tensor(0.8613, device='cuda:0', grad_fn=<NllLossBackward0>)
41 tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
42 tensor(2.0314, device='cuda:0', grad_fn=<NllLossBackward0>)
43 tensor(1.4052, device='cuda:0', grad_fn=<NllLossBackward0>)
44 tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)
45 tensor(1.0598, device='cuda:0', grad_fn=<NllLossBackward0>)
46 tensor(2.0264, device='cuda:0', grad_fn=<NllLossBackward0>)
47 tensor(1.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
48 tensor(1.5254, device='cuda:0', grad_fn=<NllLossBackward0>)
49 tensor(1.2207, device='cuda:0', grad_fn=<NllLossBackward0>)
50 tensor(1.5119, device='cuda:0', grad_fn=<NllLossBackward0>)
51 tensor(1.5717, device='cuda:0', grad_fn=<NllLossBackward0>)
52 tensor(2.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
53 tensor(1.8966, device='cuda:0', grad_fn=<NllLossBackward0>)
54 tensor(1.8331, device='cuda:0', grad_fn=<NllLossBackward0>)
55 tensor(1.5025, device='cuda:0', grad_fn=<NllLossBackward0>)
56 tensor(1.9293, device='cuda:0', grad_fn=<NllLossBackward0>)
57 tensor(1.4264, device='cuda:0', grad_fn=<NllLossBackward0>)
58 tensor(1.2697, device='cuda:0', grad_fn=<NllLossBackward0>)
59 tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
60 tensor(2.1667, device='cuda:0', grad_fn=<NllLossBackward0>)
61 tensor(1.9773, device='cuda:0', grad_fn=<NllLossBackward0>)
62 tensor(1.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
63 tensor(1.8735, device='cuda:0', grad_fn=<NllLossBackward0>)
64 tensor(1.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
65 tensor(1.8391, device='cuda:0', grad_fn=<NllLossBackward0>)
66 tensor(1.9296, device='cuda:0', grad_fn=<NllLossBackward0>)
67 tensor(1.1425, device='cuda:0', grad_fn=<NllLossBackward0>)
68 tensor(1.3241, device='cuda:0', grad_fn=<NllLossBackward0>)
69 tensor(2.0706, device='cuda:0', grad_fn=<NllLossBackward0>)
70 tensor(1.6959, device='cuda:0', grad_fn=<NllLossBackward0>)
71 tensor(2.4190, device='cuda:0', grad_fn=<NllLossBackward0>)
72 tensor(1.8708, device='cuda:0', grad_fn=<NllLossBackward0>)
73 tensor(1.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
74 tensor(1.4305, device='cuda:0', grad_fn=<NllLossBackward0>)
75 tensor(1.2618, device='cuda:0', grad_fn=<NllLossBackward0>)
76 tensor(1.2418, device='cuda:0', grad_fn=<NllLossBackward0>)
77 tensor(1.9745, device='cuda:0', grad_fn=<NllLossBackward0>)
78 tensor(1.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
79 tensor(1.8933, device='cuda:0', grad_fn=<NllLossBackward0>)
80 tensor(1.8148, device='cuda:0', grad_fn=<NllLossBackward0>)
81 tensor(1.6297, device='cuda:0', grad_fn=<NllLossBackward0>)
82 tensor(1.8356, device='cuda:0', grad_fn=<NllLossBackward0>)
83 tensor(1.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
84 tensor(0.8794, device='cuda:0', grad_fn=<NllLossBackward0>)
85 tensor(2.1694, device='cuda:0', grad_fn=<NllLossBackward0>)
86 tensor(1.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
87 tensor(1.4367, device='cuda:0', grad_fn=<NllLossBackward0>)
88 tensor(2.2893, device='cuda:0', grad_fn=<NllLossBackward0>)
89 tensor(1.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
90 tensor(1.3299, device='cuda:0', grad_fn=<NllLossBackward0>)
91 tensor(2.2562, device='cuda:0', grad_fn=<NllLossBackward0>)
92 tensor(1.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
93 tensor(2.1461, device='cuda:0', grad_fn=<NllLossBackward0>)
94 tensor(2.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
95 tensor(1.8078, device='cuda:0', grad_fn=<NllLossBackward0>)
96 tensor(1.5278, device='cuda:0', grad_fn=<NllLossBackward0>)
97 tensor(1.8288, device='cuda:0', grad_fn=<NllLossBackward0>)
98 tensor(1.3883, device='cuda:0', grad_fn=<NllLossBackward0>)
99 tensor(1.9060, device='cuda:0', grad_fn=<NllLossBackward0>)
100 tensor(2.1646, device='cuda:0', grad_fn=<NllLossBackward0>)
101 tensor(1.7988, device='cuda:0', grad_fn=<NllLossBackward0>)
102 tensor(1.5662, device='cuda:0', grad_fn=<NllLossBackward0>)
103 tensor(2.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
104 tensor(2.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
105 tensor(1.1987, device='cuda:0', grad_fn=<NllLossBackward0>)
106 tensor(2.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
107 tensor(0.8904, device='cuda:0', grad_fn=<NllLossBackward0>)
108 tensor(1.9113, device='cuda:0', grad_fn=<NllLossBackward0>)
109 tensor(1.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
110 tensor(2.2576, device='cuda:0', grad_fn=<NllLossBackward0>)
111 tensor(0.9744, device='cuda:0', grad_fn=<NllLossBackward0>)
112 tensor(2.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
113 tensor(1.9811, device='cuda:0', grad_fn=<NllLossBackward0>)
114 tensor(2.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
115 tensor(1.3393, device='cuda:0', grad_fn=<NllLossBackward0>)
116 tensor(1.8430, device='cuda:0', grad_fn=<NllLossBackward0>)
117 tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward0>)
118 tensor(1.8719, device='cuda:0', grad_fn=<NllLossBackward0>)
119 tensor(2.2339, device='cuda:0', grad_fn=<NllLossBackward0>)
120 tensor(1.9777, device='cuda:0', grad_fn=<NllLossBackward0>)
121 tensor(1.3851, device='cuda:0', grad_fn=<NllLossBackward0>)
122 tensor(1.8994, device='cuda:0', grad_fn=<NllLossBackward0>)
123 tensor(1.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
124 tensor(1.7285, device='cuda:0', grad_fn=<NllLossBackward0>)
125 tensor(2.2046, device='cuda:0', grad_fn=<NllLossBackward0>)
126 tensor(1.6408, device='cuda:0', grad_fn=<NllLossBackward0>)
127 tensor(1.8793, device='cuda:0', grad_fn=<NllLossBackward0>)
128 tensor(1.7873, device='cuda:0', grad_fn=<NllLossBackward0>)
129 tensor(1.7017, device='cuda:0', grad_fn=<NllLossBackward0>)
130 tensor(1.8766, device='cuda:0', grad_fn=<NllLossBackward0>)
131 tensor(1.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
132 tensor(1.8093, device='cuda:0', grad_fn=<NllLossBackward0>)
133 tensor(1.9524, device='cuda:0', grad_fn=<NllLossBackward0>)
134 tensor(1.3639, device='cuda:0', grad_fn=<NllLossBackward0>)
135 tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
136 tensor(1.6426, device='cuda:0', grad_fn=<NllLossBackward0>)
137 tensor(2.0888, device='cuda:0', grad_fn=<NllLossBackward0>)
138 tensor(1.4289, device='cuda:0', grad_fn=<NllLossBackward0>)
139 tensor(2.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
140 tensor(1.9939, device='cuda:0', grad_fn=<NllLossBackward0>)
141 tensor(1.5608, device='cuda:0', grad_fn=<NllLossBackward0>)
142 tensor(1.8946, device='cuda:0', grad_fn=<NllLossBackward0>)
143 tensor(1.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
144 tensor(1.9007, device='cuda:0', grad_fn=<NllLossBackward0>)
145 tensor(1.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
146 tensor(1.8179, device='cuda:0', grad_fn=<NllLossBackward0>)
147 tensor(1.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
148 tensor(1.3431, device='cuda:0', grad_fn=<NllLossBackward0>)
149 tensor(1.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
150 tensor(1.3689, device='cuda:0', grad_fn=<NllLossBackward0>)
151 tensor(1.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
152 tensor(1.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
153 tensor(1.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
154 tensor(2.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
155 tensor(2.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
156 tensor(1.8862, device='cuda:0', grad_fn=<NllLossBackward0>)
157 tensor(1.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
158 tensor(1.9145, device='cuda:0', grad_fn=<NllLossBackward0>)
159 tensor(2.1954, device='cuda:0', grad_fn=<NllLossBackward0>)
160 tensor(2.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
161 tensor(1.9597, device='cuda:0', grad_fn=<NllLossBackward0>)
162 tensor(1.8344, device='cuda:0', grad_fn=<NllLossBackward0>)
163 tensor(2.2154, device='cuda:0', grad_fn=<NllLossBackward0>)
164 tensor(1.8312, device='cuda:0', grad_fn=<NllLossBackward0>)
165 tensor(2.2496, device='cuda:0', grad_fn=<NllLossBackward0>)
166 tensor(1.9930, device='cuda:0', grad_fn=<NllLossBackward0>)
167 tensor(2.2032, device='cuda:0', grad_fn=<NllLossBackward0>)
168 tensor(2.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
169 tensor(1.9094, device='cuda:0', grad_fn=<NllLossBackward0>)
170 tensor(1.4667, device='cuda:0', grad_fn=<NllLossBackward0>)
171 tensor(1.5188, device='cuda:0', grad_fn=<NllLossBackward0>)
172 tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)
173 tensor(1.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
174 tensor(1.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
175 tensor(2.2110, device='cuda:0', grad_fn=<NllLossBackward0>)
176 tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
177 tensor(1.4045, device='cuda:0', grad_fn=<NllLossBackward0>)
178 tensor(1.9232, device='cuda:0', grad_fn=<NllLossBackward0>)
179 tensor(2.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
180 tensor(1.3202, device='cuda:0', grad_fn=<NllLossBackward0>)
181 tensor(1.9493, device='cuda:0', grad_fn=<NllLossBackward0>)
182 tensor(1.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
183 tensor(1.2472, device='cuda:0', grad_fn=<NllLossBackward0>)
184 tensor(2.4029, device='cuda:0', grad_fn=<NllLossBackward0>)
185 tensor(1.5245, device='cuda:0', grad_fn=<NllLossBackward0>)
186 tensor(2.0014, device='cuda:0', grad_fn=<NllLossBackward0>)
187 tensor(2.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
188 tensor(2.5731, device='cuda:0', grad_fn=<NllLossBackward0>)
189 tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)
190 tensor(1.2640, device='cuda:0', grad_fn=<NllLossBackward0>)
191 tensor(1.9164, device='cuda:0', grad_fn=<NllLossBackward0>)
192 tensor(2.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
193 tensor(2.1157, device='cuda:0', grad_fn=<NllLossBackward0>)
194 tensor(2.0712, device='cuda:0', grad_fn=<NllLossBackward0>)
195 tensor(1.8279, device='cuda:0', grad_fn=<NllLossBackward0>)
196 tensor(1.5255, device='cuda:0', grad_fn=<NllLossBackward0>)
197 tensor(1.3430, device='cuda:0', grad_fn=<NllLossBackward0>)
198 tensor(2.0745, device='cuda:0', grad_fn=<NllLossBackward0>)
199 tensor(1.2687, device='cuda:0', grad_fn=<NllLossBackward0>)
200 tensor(1.9182, device='cuda:0', grad_fn=<NllLossBackward0>)
201 tensor(2.2753, device='cuda:0', grad_fn=<NllLossBackward0>)
202 tensor(2.0397, device='cuda:0', grad_fn=<NllLossBackward0>)
203 tensor(2.4037, device='cuda:0', grad_fn=<NllLossBackward0>)
204 tensor(1.9811, device='cuda:0', grad_fn=<NllLossBackward0>)
205 tensor(1.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
206 tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
207 tensor(2.0520, device='cuda:0', grad_fn=<NllLossBackward0>)
208 tensor(1.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
209 tensor(1.6594, device='cuda:0', grad_fn=<NllLossBackward0>)
210 tensor(2.2121, device='cuda:0', grad_fn=<NllLossBackward0>)
211 tensor(1.3411, device='cuda:0', grad_fn=<NllLossBackward0>)
212 tensor(1.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
213 tensor(1.8143, device='cuda:0', grad_fn=<NllLossBackward0>)
214 tensor(1.9968, device='cuda:0', grad_fn=<NllLossBackward0>)
215 tensor(2.2746, device='cuda:0', grad_fn=<NllLossBackward0>)
216 tensor(1.8932, device='cuda:0', grad_fn=<NllLossBackward0>)
217 tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
218 tensor(2.0474, device='cuda:0', grad_fn=<NllLossBackward0>)
219 tensor(1.5614, device='cuda:0', grad_fn=<NllLossBackward0>)
220 tensor(2.0223, device='cuda:0', grad_fn=<NllLossBackward0>)
221 tensor(0.9459, device='cuda:0', grad_fn=<NllLossBackward0>)
222 tensor(1.5372, device='cuda:0', grad_fn=<NllLossBackward0>)
223 tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward0>)
224 tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
225 tensor(1.8603, device='cuda:0', grad_fn=<NllLossBackward0>)
226 tensor(1.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
227 tensor(2.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
228 tensor(0.8308, device='cuda:0', grad_fn=<NllLossBackward0>)
229 tensor(1.8788, device='cuda:0', grad_fn=<NllLossBackward0>)
230 tensor(2.1702, device='cuda:0', grad_fn=<NllLossBackward0>)
231 tensor(2.1882, device='cuda:0', grad_fn=<NllLossBackward0>)
232 tensor(1.9887, device='cuda:0', grad_fn=<NllLossBackward0>)
233 tensor(1.4886, device='cuda:0', grad_fn=<NllLossBackward0>)
234 tensor(1.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
235 tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
236 tensor(1.5500, device='cuda:0', grad_fn=<NllLossBackward0>)
237 tensor(1.8474, device='cuda:0', grad_fn=<NllLossBackward0>)
238 tensor(1.8798, device='cuda:0', grad_fn=<NllLossBackward0>)
239 tensor(1.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
240 tensor(1.8984, device='cuda:0', grad_fn=<NllLossBackward0>)
241 tensor(2.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
242 tensor(1.7168, device='cuda:0', grad_fn=<NllLossBackward0>)
243 tensor(2.4589, device='cuda:0', grad_fn=<NllLossBackward0>)
244 tensor(1.8297, device='cuda:0', grad_fn=<NllLossBackward0>)
245 tensor(1.1484, device='cuda:0', grad_fn=<NllLossBackward0>)
246 tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
247 tensor(1.4908, device='cuda:0', grad_fn=<NllLossBackward0>)
248 tensor(1.6903, device='cuda:0', grad_fn=<NllLossBackward0>)
249 tensor(1.2196, device='cuda:0', grad_fn=<NllLossBackward0>)
250 tensor(1.8958, device='cuda:0', grad_fn=<NllLossBackward0>)
251 tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
252 tensor(2.0203, device='cuda:0', grad_fn=<NllLossBackward0>)
253 tensor(1.1065, device='cuda:0', grad_fn=<NllLossBackward0>)
254 tensor(1.9654, device='cuda:0', grad_fn=<NllLossBackward0>)
255 tensor(2.0854, device='cuda:0', grad_fn=<NllLossBackward0>)
sanity check for i_norm and o_norm
i_norm:  torch.Size([4096]) tensor([0.3051, 0.0677, 0.0027,  ..., 0.0376, 0.0432, 0.0179], device='cuda:0')
o_norm:  torch.Size([4096]) tensor([4.9002e-06, 3.3027e-06, 6.5066e-06,  ..., 8.6431e-04, 1.7183e-04,
        8.0291e-05], device='cuda:0')
Starting layer-wise PTQ...
collecting activations...
Ready.
collecting FP target output activations for block 0...
0 self_attn.q_proj
Factorizing 0.self_attn.q_proj...
err 259 0.0036176706198602915 623.6671142578125
sparsity check:  1.5 bpw
relative err after factorization:  0.2427748515401025 740.0421752929688 

0 self_attn.v_proj
Factorizing 0.self_attn.v_proj...
err 259 0.07083207368850708 54.13825607299805
sparsity check:  1.5 bpw
relative err after factorization:  0.2184992359766279 447.8988952636719 

0 self_attn.o_proj
Factorizing 0.self_attn.o_proj...
err 259 0.2708389461040497 862.3992919921875
sparsity check:  1.5 bpw
relative err after factorization:  0.1940139690661777 167.80865478515625 

0 self_attn.k_proj
err before 6.878810368959876e-05
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd66af4ed0>
epoch: 0, train err: 7.35237842945935e-05
epoch: 1, train err: 3.3731586803753544e-05
epoch: 2, train err: 3.313194387999374e-05
epoch: 3, train err: 2.421306828637171e-05
Factorizing 0.self_attn.k_proj...
err 259 0.0016067095566540956 67.40940856933594
sparsity check:  1.5 bpw
relative err after factorization:  0.12557329930513636 483.098876953125 

0 mlp.up_proj
Factorizing 0.mlp.up_proj...
err 259 1.0780017375946045 114.7215576171875
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1657670390254552 1945.69580078125 

0 mlp.gate_proj
Factorizing 0.mlp.gate_proj...
err 259 0.9329674243927002 104.95792388916016
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.157594405745105 1913.93896484375 

0 mlp.down_proj
Factorizing 0.mlp.down_proj...
err 259 5.824599266052246 181.74281311035156
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19382143776571192 2475.534912109375 

collecting Q input activations for next block ...
collecting FP target output activations for block 1...
1 self_attn.q_proj
err before 0.17329000151949003
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd6488e0d0>
epoch: 0, train err: 0.06575826500829862
epoch: 1, train err: 0.10439631171630026
epoch: 2, train err: 0.02540376031311098
epoch: 3, train err: 0.007662113912829227
Factorizing 1.self_attn.q_proj...
err 259 0.15186476707458496 498.89495849609375
sparsity check:  1.5 bpw
relative err after factorization:  0.10681528940709152 1254.6160888671875 

1 self_attn.v_proj
Factorizing 1.self_attn.v_proj...
err 259 2.3403971195220947 132.9949951171875
sparsity check:  1.5 bpw
relative err after factorization:  0.2656341540188752 449.75811767578125 

1 self_attn.o_proj
Factorizing 1.self_attn.o_proj...
err 259 4.636320114135742 413.9527282714844
sparsity check:  1.5 bpw
relative err after factorization:  0.15351785250204641 175.29600524902344 

1 self_attn.k_proj
err before 0.036964007591450354
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd648a1010>
epoch: 0, train err: 0.022822140248536016
epoch: 1, train err: 0.017036852081218967
epoch: 2, train err: 0.01427067426902795
epoch: 3, train err: 0.013818876875120623
Factorizing 1.self_attn.k_proj...
err 259 0.14331120252609253 1179.085693359375
sparsity check:  1.5 bpw
relative err after factorization:  0.10707177028708698 1254.895263671875 

1 mlp.up_proj
Factorizing 1.mlp.up_proj...
err 259 10.3445463180542 87.61893463134766
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16057110017064455 2166.7080078125 

1 mlp.gate_proj
Factorizing 1.mlp.gate_proj...
err 259 6.738584518432617 69.15704345703125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.135672999507005 2073.1494140625 

1 mlp.down_proj
Factorizing 1.mlp.down_proj...
err 259 44.40193176269531 2382889.25
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2388699761706751 3332.26416015625 

collecting Q input activations for next block ...
collecting FP target output activations for block 2...
2 self_attn.q_proj
err before 0.03701938440644881
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd6488e510>
epoch: 0, train err: 0.029419883405353175
epoch: 1, train err: 0.025817867783189286
epoch: 2, train err: 0.02481372699548956
epoch: 3, train err: 0.024521435305359773
Factorizing 2.self_attn.q_proj...
err 259 0.8989889621734619 112.14537048339844
sparsity check:  1.5 bpw
relative err after factorization:  0.12873609633274588 1537.513427734375 

2 self_attn.v_proj
Factorizing 2.self_attn.v_proj...
err 259 836.3692626953125 7279.423828125
sparsity check:  1.5 bpw
relative err after factorization:  0.17796810718316525 615.804931640625 

2 self_attn.o_proj
Factorizing 2.self_attn.o_proj...
err 259 16.21461296081543 143.85475158691406
sparsity check:  1.5 bpw
relative err after factorization:  0.21096878648996673 686.700927734375 

2 self_attn.k_proj
err before 0.029960860720166238
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64844e90>
epoch: 0, train err: 0.02783196864402271
epoch: 1, train err: 0.026504339864914073
epoch: 2, train err: 0.02603998317499645
epoch: 3, train err: 0.025894062277075136
Factorizing 2.self_attn.k_proj...
err 259 1.0049206018447876 256.8680725097656
sparsity check:  1.5 bpw
relative err after factorization:  0.11825669681586744 1573.4473876953125 

2 mlp.up_proj
Factorizing 2.mlp.up_proj...
err 259 25.219051361083984 162.1163330078125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17057230539455992 2382.287109375 

2 mlp.gate_proj
Factorizing 2.mlp.gate_proj...
err 259 18.85409164428711 136.89744567871094
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15494045179269147 2495.46728515625 

2 mlp.down_proj
Factorizing 2.mlp.down_proj...
err 259 42.487003326416016 271.6099548339844
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1898559929594713 2679.76171875 

collecting Q input activations for next block ...
collecting FP target output activations for block 3...
3 self_attn.q_proj
err before 0.03861917540052673
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba646fd0>
epoch: 0, train err: 0.0318384371130378
epoch: 1, train err: 0.02839363271050388
epoch: 2, train err: 0.02744100775453262
epoch: 3, train err: 0.02715186031491612
Factorizing 3.self_attn.q_proj...
err 259 2.957970380783081 172.2089080810547
sparsity check:  1.5 bpw
relative err after factorization:  0.16090083745344913 1717.50927734375 

3 self_attn.v_proj
Factorizing 3.self_attn.v_proj...
err 259 1308.0867919921875 9559.6689453125
sparsity check:  1.5 bpw
relative err after factorization:  0.18108013274635285 563.946044921875 

3 self_attn.o_proj
Factorizing 3.self_attn.o_proj...
err 259 17.45654296875 166.61410522460938
sparsity check:  1.5 bpw
relative err after factorization:  0.19511112563990565 582.1390991210938 

3 self_attn.k_proj
err before 0.035361811867915094
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd648a8510>
epoch: 0, train err: 0.031214109578286298
epoch: 1, train err: 0.029331318524782546
epoch: 2, train err: 0.028810993368097115
epoch: 3, train err: 0.028647805491345935
Factorizing 3.self_attn.k_proj...
err 259 3.4642865657806396 334.45428466796875
sparsity check:  1.5 bpw
relative err after factorization:  0.1616748800379515 1875.7481689453125 

3 mlp.up_proj
Factorizing 3.mlp.up_proj...
err 259 34.731266021728516 209.4544677734375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18322173897975189 2560.74853515625 

3 mlp.gate_proj
Factorizing 3.mlp.gate_proj...
err 259 29.100421905517578 186.87847900390625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17355251164992272 2839.43603515625 

3 mlp.down_proj
Factorizing 3.mlp.down_proj...
err 259 45.44549560546875 292.89105224609375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1812764423890401 2537.13818359375 

collecting Q input activations for next block ...
collecting FP target output activations for block 4...
4 self_attn.q_proj
err before 0.03418502709973836
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba646fd0>
epoch: 0, train err: 0.029541994612372946
epoch: 1, train err: 0.02675693367200438
epoch: 2, train err: 0.025872301004710607
epoch: 3, train err: 0.025579837063560262
Factorizing 4.self_attn.q_proj...
err 259 2.42588210105896 163.04364013671875
sparsity check:  1.5 bpw
relative err after factorization:  0.15788243949969177 1829.8642578125 

4 self_attn.v_proj
Factorizing 4.self_attn.v_proj...
err 259 753.6091918945312 5293.20068359375
sparsity check:  1.5 bpw
relative err after factorization:  0.18115081051361165 627.576904296875 

4 self_attn.o_proj
Factorizing 4.self_attn.o_proj...
err 259 21.94965362548828 170.3740234375
sparsity check:  1.5 bpw
relative err after factorization:  0.19340838748684436 641.26416015625 

4 self_attn.k_proj
err before 0.03272296283830656
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd648b2ad0>
epoch: 0, train err: 0.02963022965559503
epoch: 1, train err: 0.02795275305834366
epoch: 2, train err: 0.027421993741882034
epoch: 3, train err: 0.027252081294136588
Factorizing 4.self_attn.k_proj...
err 259 2.821540355682373 324.973388671875
sparsity check:  1.5 bpw
relative err after factorization:  0.15846717676413 1922.455078125 

4 mlp.up_proj
Factorizing 4.mlp.up_proj...
err 259 37.399898529052734 239.11550903320312
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18672206177515427 2561.701416015625 

4 mlp.gate_proj
Factorizing 4.mlp.gate_proj...
err 259 30.070032119750977 221.35659790039062
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17411022447386962 2958.7822265625 

4 mlp.down_proj
Factorizing 4.mlp.down_proj...
err 259 48.354209899902344 330.61578369140625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18214941999000517 2480.221923828125 

collecting Q input activations for next block ...
collecting FP target output activations for block 5...
5 self_attn.q_proj
err before 0.03688701891951496
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd66aa4ad0>
epoch: 0, train err: 0.03259758000058355
epoch: 1, train err: 0.030035876523470506
epoch: 2, train err: 0.029194701521191746
epoch: 3, train err: 0.02892427935876185
Factorizing 5.self_attn.q_proj...
err 259 2.5088233947753906 144.46011352539062
sparsity check:  1.5 bpw
relative err after factorization:  0.1636176444767566 1914.694580078125 

5 self_attn.v_proj
Factorizing 5.self_attn.v_proj...
err 259 515.7334594726562 3545.54541015625
sparsity check:  1.5 bpw
relative err after factorization:  0.18127576626245084 660.190673828125 

5 self_attn.o_proj
Factorizing 5.self_attn.o_proj...
err 259 20.367372512817383 181.52163696289062
sparsity check:  1.5 bpw
relative err after factorization:  0.19880069911912085 692.54150390625 

5 self_attn.k_proj
err before 0.035271703054604586
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64911a90>
epoch: 0, train err: 0.032810768541821744
epoch: 1, train err: 0.03131345540168695
epoch: 2, train err: 0.030805782582319807
epoch: 3, train err: 0.03064116522728
Factorizing 5.self_attn.k_proj...
err 259 2.7821547985076904 274.843505859375
sparsity check:  1.5 bpw
relative err after factorization:  0.1603304378147446 2041.262939453125 

5 mlp.up_proj
Factorizing 5.mlp.up_proj...
err 259 38.24570846557617 235.3563995361328
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18623059818948726 2553.1953125 

5 mlp.gate_proj
Factorizing 5.mlp.gate_proj...
err 259 29.138835906982422 203.56039428710938
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1720561135188322 2957.2685546875 

5 mlp.down_proj
Factorizing 5.mlp.down_proj...
err 259 44.89494323730469 278.74114990234375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17839479027139024 2433.98193359375 

collecting Q input activations for next block ...
collecting FP target output activations for block 6...
6 self_attn.q_proj
err before 0.037735466423328035
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba620210>
epoch: 0, train err: 0.033521377699798904
epoch: 1, train err: 0.031233207337209024
epoch: 2, train err: 0.03046985744003905
epoch: 3, train err: 0.030190329758625012
Factorizing 6.self_attn.q_proj...
err 259 3.070543050765991 170.01116943359375
sparsity check:  1.5 bpw
relative err after factorization:  0.15043780006931526 1565.44384765625 

6 self_attn.v_proj
Factorizing 6.self_attn.v_proj...
err 259 334.8837890625 2560.8115234375
sparsity check:  1.5 bpw
relative err after factorization:  0.17670924848928699 553.2694091796875 

6 self_attn.o_proj
Factorizing 6.self_attn.o_proj...
err 259 18.340038299560547 154.83154296875
sparsity check:  1.5 bpw
relative err after factorization:  0.18092661683551867 550.0557861328125 

6 self_attn.k_proj
err before 0.034978140058228746
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd6485ccd0>
epoch: 0, train err: 0.03302398879895918
epoch: 1, train err: 0.03173988980415743
epoch: 2, train err: 0.031282575051591266
epoch: 3, train err: 0.03113057695009047
Factorizing 6.self_attn.k_proj...
err 259 4.008009910583496 303.5687561035156
sparsity check:  1.5 bpw
relative err after factorization:  0.15279856870486452 1659.1925048828125 

6 mlp.up_proj
Factorizing 6.mlp.up_proj...
err 259 38.71955490112305 243.40423583984375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18694860766079152 2536.44677734375 

6 mlp.gate_proj
Factorizing 6.mlp.gate_proj...
err 259 27.92485809326172 196.15675354003906
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1707721304401668 3024.6689453125 

6 mlp.down_proj
Factorizing 6.mlp.down_proj...
err 259 43.273712158203125 273.7005920410156
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1778844644953107 2391.112548828125 

collecting Q input activations for next block ...
collecting FP target output activations for block 7...
7 self_attn.q_proj
err before 0.03714961624064017
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68768450>
epoch: 0, train err: 0.03350586064334493
epoch: 1, train err: 0.03149605892394902
epoch: 2, train err: 0.030806780720013194
epoch: 3, train err: 0.03056746432412183
Factorizing 7.self_attn.q_proj...
err 259 2.9555954933166504 161.39266967773438
sparsity check:  1.5 bpw
relative err after factorization:  0.15250413884465566 1580.30224609375 

7 self_attn.v_proj
Factorizing 7.self_attn.v_proj...
err 259 247.1497802734375 1993.6248779296875
sparsity check:  1.5 bpw
relative err after factorization:  0.17745464952749512 569.476318359375 

7 self_attn.o_proj
Factorizing 7.self_attn.o_proj...
err 259 17.40481948852539 150.04022216796875
sparsity check:  1.5 bpw
relative err after factorization:  0.19265738293889517 594.4493408203125 

7 self_attn.k_proj
err before 0.034810215584002435
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd648ec3d0>
epoch: 0, train err: 0.033196462092746515
epoch: 1, train err: 0.03204877661482897
epoch: 2, train err: 0.03163454986497527
epoch: 3, train err: 0.031496129937295336
Factorizing 7.self_attn.k_proj...
err 259 3.750109910964966 282.48541259765625
sparsity check:  1.5 bpw
relative err after factorization:  0.15382027818253974 1615.092041015625 

7 mlp.up_proj
Factorizing 7.mlp.up_proj...
err 259 35.832557678222656 233.78427124023438
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18550311276404205 2532.701171875 

7 mlp.gate_proj
Factorizing 7.mlp.gate_proj...
err 259 25.319782257080078 185.77584838867188
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16846051750695085 2980.2666015625 

7 mlp.down_proj
Factorizing 7.mlp.down_proj...
err 259 39.599510192871094 253.41090393066406
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17559060444803629 2366.215087890625 

collecting Q input activations for next block ...
collecting FP target output activations for block 8...
8 self_attn.q_proj
err before 0.03877158097748179
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68769290>
epoch: 0, train err: 0.035236454525147565
epoch: 1, train err: 0.033245838319999166
epoch: 2, train err: 0.032557256148720626
epoch: 3, train err: 0.032333020368241705
Factorizing 8.self_attn.q_proj...
err 259 2.9082796573638916 152.5902557373047
sparsity check:  1.5 bpw
relative err after factorization:  0.1599418978047673 1689.753662109375 

8 self_attn.v_proj
Factorizing 8.self_attn.v_proj...
err 259 171.75704956054688 1396.1634521484375
sparsity check:  1.5 bpw
relative err after factorization:  0.17382152227865122 588.1123046875 

8 self_attn.o_proj
Factorizing 8.self_attn.o_proj...
err 259 18.576438903808594 157.70675659179688
sparsity check:  1.5 bpw
relative err after factorization:  0.1948433593858491 637.1402587890625 

8 self_attn.k_proj
err before 0.036493261039140634
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd648efa50>
epoch: 0, train err: 0.03503829582768958
epoch: 1, train err: 0.033944665075978264
epoch: 2, train err: 0.033548807419720106
epoch: 3, train err: 0.033416380640119314
Factorizing 8.self_attn.k_proj...
err 259 3.4386844635009766 304.29052734375
sparsity check:  1.5 bpw
relative err after factorization:  0.1656582858112825 1776.493896484375 

8 mlp.up_proj
Factorizing 8.mlp.up_proj...
err 259 31.36800193786621 206.91880798339844
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18393322384513244 2570.21533203125 

8 mlp.gate_proj
Factorizing 8.mlp.gate_proj...
err 259 22.410785675048828 164.26626586914062
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1676519600652723 2866.28662109375 

8 mlp.down_proj
Factorizing 8.mlp.down_proj...
err 259 35.828147888183594 230.2300567626953
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17385466427123597 2389.10791015625 

collecting Q input activations for next block ...
collecting FP target output activations for block 9...
9 self_attn.q_proj
err before 0.04158231396286283
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd67307310>
epoch: 0, train err: 0.03826430006301962
epoch: 1, train err: 0.03637515027367044
epoch: 2, train err: 0.035722771819564514
epoch: 3, train err: 0.035501554142683744
Factorizing 9.self_attn.q_proj...
err 259 3.3848860263824463 130.3597869873047
sparsity check:  1.5 bpw
relative err after factorization:  0.1634447317075421 1730.1917724609375 

9 self_attn.v_proj
Factorizing 9.self_attn.v_proj...
err 259 147.00979614257812 1211.349853515625
sparsity check:  1.5 bpw
relative err after factorization:  0.17441118882982617 613.5146484375 

9 self_attn.o_proj
Factorizing 9.self_attn.o_proj...
err 259 19.12118911743164 153.35882568359375
sparsity check:  1.5 bpw
relative err after factorization:  0.19664940042534051 672.009765625 

9 self_attn.k_proj
err before 0.039746813774399925
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64636d90>
epoch: 0, train err: 0.03834586198354373
epoch: 1, train err: 0.03726873562845867
epoch: 2, train err: 0.036877149977954105
epoch: 3, train err: 0.03674617914657574
Factorizing 9.self_attn.k_proj...
err 259 3.825430393218994 287.58526611328125
sparsity check:  1.5 bpw
relative err after factorization:  0.16157971558169695 1803.6474609375 

9 mlp.up_proj
Factorizing 9.mlp.up_proj...
err 259 27.608318328857422 199.9243927001953
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1830894231214625 2593.907958984375 

9 mlp.gate_proj
Factorizing 9.mlp.gate_proj...
err 259 19.91997718811035 148.11260986328125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16791063089805064 2816.406005859375 

9 mlp.down_proj
Factorizing 9.mlp.down_proj...
err 259 33.22833251953125 212.28878784179688
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17372404218672186 2422.04736328125 

collecting Q input activations for next block ...
collecting FP target output activations for block 10...
10 self_attn.q_proj
err before 0.04501546247047372
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd66b1b210>
epoch: 0, train err: 0.041505397559376433
epoch: 1, train err: 0.03946735076897312
epoch: 2, train err: 0.03878414232167415
epoch: 3, train err: 0.03855725593166426
Factorizing 10.self_attn.q_proj...
err 259 3.1410632133483887 116.44744110107422
sparsity check:  1.5 bpw
relative err after factorization:  0.15211544252416645 1604.2650146484375 

10 self_attn.v_proj
Factorizing 10.self_attn.v_proj...
err 259 89.55984497070312 717.2581787109375
sparsity check:  1.5 bpw
relative err after factorization:  0.16965264431127622 584.3533325195312 

10 self_attn.o_proj
Factorizing 10.self_attn.o_proj...
err 259 21.247604370117188 169.80873107910156
sparsity check:  1.5 bpw
relative err after factorization:  0.17787326934682757 601.9175415039062 

10 self_attn.k_proj
err before 0.04304131501703523
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64661f10>
epoch: 0, train err: 0.041580817676731385
epoch: 1, train err: 0.04050122595799621
epoch: 2, train err: 0.040099834091961384
epoch: 3, train err: 0.03996568828006275
Factorizing 10.self_attn.k_proj...
err 259 3.4169862270355225 273.05999755859375
sparsity check:  1.5 bpw
relative err after factorization:  0.15101719694813964 1696.303466796875 

10 mlp.up_proj
Factorizing 10.mlp.up_proj...
err 259 25.828426361083984 175.10865783691406
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18288850598639655 2647.73583984375 

10 mlp.gate_proj
Factorizing 10.mlp.gate_proj...
err 259 18.469886779785156 139.67080688476562
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16831781791024 2805.721923828125 

10 mlp.down_proj
Factorizing 10.mlp.down_proj...
err 259 32.624366760253906 207.8417205810547
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1721093890172475 2443.090087890625 

collecting Q input activations for next block ...
collecting FP target output activations for block 11...
11 self_attn.q_proj
err before 0.045329547763685696
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd687841d0>
epoch: 0, train err: 0.04224575919943163
epoch: 1, train err: 0.040322042572370265
epoch: 2, train err: 0.03965199882804882
epoch: 3, train err: 0.039435055005014874
Factorizing 11.self_attn.q_proj...
err 259 2.9856016635894775 124.05208587646484
sparsity check:  1.5 bpw
relative err after factorization:  0.1432619242195754 1363.395751953125 

11 self_attn.v_proj
Factorizing 11.self_attn.v_proj...
err 259 98.19975280761719 839.3353881835938
sparsity check:  1.5 bpw
relative err after factorization:  0.1668817504484268 624.7766723632812 

11 self_attn.o_proj
Factorizing 11.self_attn.o_proj...
err 259 18.609527587890625 153.1534881591797
sparsity check:  1.5 bpw
relative err after factorization:  0.17267696603591684 633.1920166015625 

11 self_attn.k_proj
err before 0.042996107556973584
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64681250>
epoch: 0, train err: 0.04194088879739866
epoch: 1, train err: 0.040972027149109636
epoch: 2, train err: 0.040595108905108646
epoch: 3, train err: 0.04046761550125666
Factorizing 11.self_attn.k_proj...
err 259 3.3021597862243652 282.2302551269531
sparsity check:  1.5 bpw
relative err after factorization:  0.15028174891804133 1425.330810546875 

11 mlp.up_proj
Factorizing 11.mlp.up_proj...
err 259 24.63333511352539 164.5595703125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1816246411347736 2673.99609375 

11 mlp.gate_proj
Factorizing 11.mlp.gate_proj...
err 259 17.421632766723633 127.3909912109375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1666859825289354 2757.10986328125 

11 mlp.down_proj
Factorizing 11.mlp.down_proj...
err 259 31.64636993408203 199.18017578125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17407214531260734 2504.353515625 

collecting Q input activations for next block ...
collecting FP target output activations for block 12...
12 self_attn.q_proj
err before 0.0468590350355953
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd66a34d50>
epoch: 0, train err: 0.044118214762420394
epoch: 1, train err: 0.04230251768603921
epoch: 2, train err: 0.04165306207141839
epoch: 3, train err: 0.04144279578758869
Factorizing 12.self_attn.q_proj...
err 259 3.14056134223938 106.05729675292969
sparsity check:  1.5 bpw
relative err after factorization:  0.16387678754078083 1625.491455078125 

12 self_attn.v_proj
Factorizing 12.self_attn.v_proj...
err 259 86.72294616699219 720.04833984375
sparsity check:  1.5 bpw
relative err after factorization:  0.17476193863711215 638.1566162109375 

12 self_attn.o_proj
Factorizing 12.self_attn.o_proj...
err 259 19.12181854248047 143.41961669921875
sparsity check:  1.5 bpw
relative err after factorization:  0.18589203288798234 666.3782348632812 

12 self_attn.k_proj
err before 0.04494222322682617
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd646a0690>
epoch: 0, train err: 0.0440091656128061
epoch: 1, train err: 0.043075010398752056
epoch: 2, train err: 0.04270390824240167
epoch: 3, train err: 0.0425779396755388
Factorizing 12.self_attn.k_proj...
err 259 3.1820473670959473 233.3356475830078
sparsity check:  1.5 bpw
relative err after factorization:  0.15764179384681137 1666.275146484375 

12 mlp.up_proj
Factorizing 12.mlp.up_proj...
err 259 24.462068557739258 162.13677978515625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18115225106744234 2713.9833984375 

12 mlp.gate_proj
Factorizing 12.mlp.gate_proj...
err 259 17.59726333618164 126.26746368408203
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.167371849046008 2731.25 

12 mlp.down_proj
Factorizing 12.mlp.down_proj...
err 259 31.668964385986328 198.58770751953125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17472502717811925 2554.13232421875 

collecting Q input activations for next block ...
collecting FP target output activations for block 13...
13 self_attn.q_proj
err before 0.05030615885334555
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68784f90>
epoch: 0, train err: 0.04729595915705431
epoch: 1, train err: 0.04535631437465781
epoch: 2, train err: 0.04465666516625788
epoch: 3, train err: 0.04443037313467357
Factorizing 13.self_attn.q_proj...
err 259 3.015279769897461 96.27314758300781
sparsity check:  1.5 bpw
relative err after factorization:  0.16569635369528798 1613.395751953125 

13 self_attn.v_proj
Factorizing 13.self_attn.v_proj...
err 259 89.51664733886719 685.13916015625
sparsity check:  1.5 bpw
relative err after factorization:  0.18391993905989476 722.03173828125 

13 self_attn.o_proj
Factorizing 13.self_attn.o_proj...
err 259 18.45638656616211 141.68954467773438
sparsity check:  1.5 bpw
relative err after factorization:  0.18242586843391204 701.2520751953125 

13 self_attn.k_proj
err before 0.04842429602285847
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd646a3490>
epoch: 0, train err: 0.04721721888927277
epoch: 1, train err: 0.04615381805342622
epoch: 2, train err: 0.04574651764414739
epoch: 3, train err: 0.04560921009397134
Factorizing 13.self_attn.k_proj...
err 259 3.488931655883789 213.41763305664062
sparsity check:  1.5 bpw
relative err after factorization:  0.16629397859957673 1682.4244384765625 

13 mlp.up_proj
Factorizing 13.mlp.up_proj...
err 259 25.69830894470215 181.94918823242188
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1833103152515546 2804.682373046875 

13 mlp.gate_proj
Factorizing 13.mlp.gate_proj...
err 259 18.455615997314453 132.59503173828125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1698103396875579 2752.3291015625 

13 mlp.down_proj
Factorizing 13.mlp.down_proj...
err 259 35.276817321777344 221.40216064453125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17877776944067814 2657.187255859375 

collecting Q input activations for next block ...
collecting FP target output activations for block 14...
14 self_attn.q_proj
err before 0.053230037054163404
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68787610>
epoch: 0, train err: 0.05048838976654224
epoch: 1, train err: 0.04847674036864191
epoch: 2, train err: 0.04774513130541891
epoch: 3, train err: 0.047504647583991755
Factorizing 14.self_attn.q_proj...
err 259 2.9052681922912598 89.24067687988281
sparsity check:  1.5 bpw
relative err after factorization:  0.15523423601163333 1502.963623046875 

14 self_attn.v_proj
Factorizing 14.self_attn.v_proj...
err 259 71.90408325195312 519.7965698242188
sparsity check:  1.5 bpw
relative err after factorization:  0.17832302578462222 673.9813232421875 

14 self_attn.o_proj
Factorizing 14.self_attn.o_proj...
err 259 20.064010620117188 147.97134399414062
sparsity check:  1.5 bpw
relative err after factorization:  0.17976893771272098 663.4136962890625 

14 self_attn.k_proj
err before 0.05121409526327625
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd646be910>
epoch: 0, train err: 0.05029657037812285
epoch: 1, train err: 0.04926710394647671
epoch: 2, train err: 0.04884440125169931
epoch: 3, train err: 0.04870087088056607
Factorizing 14.self_attn.k_proj...
err 259 2.9439663887023926 200.50741577148438
sparsity check:  1.5 bpw
relative err after factorization:  0.15383089386425963 1551.25146484375 

14 mlp.up_proj
Factorizing 14.mlp.up_proj...
err 259 26.110536575317383 172.24832153320312
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1816078600165902 2784.56494140625 

14 mlp.gate_proj
Factorizing 14.mlp.gate_proj...
err 259 19.369667053222656 135.8104248046875
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16875490800308302 2723.15576171875 

14 mlp.down_proj
Factorizing 14.mlp.down_proj...
err 259 36.920684814453125 231.5574188232422
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18033313146964067 2686.4208984375 

collecting Q input activations for next block ...
collecting FP target output activations for block 15...
15 self_attn.q_proj
err before 0.0589629504684126
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68625710>
epoch: 0, train err: 0.055766396995750256
epoch: 1, train err: 0.053565263020573184
epoch: 2, train err: 0.05275722226360813
epoch: 3, train err: 0.05249188916059211
Factorizing 15.self_attn.q_proj...
err 259 3.3313379287719727 105.59378051757812
sparsity check:  1.5 bpw
relative err after factorization:  0.16518402049692094 1560.66943359375 

15 self_attn.v_proj
Factorizing 15.self_attn.v_proj...
err 259 80.818603515625 569.32373046875
sparsity check:  1.5 bpw
relative err after factorization:  0.18060660859130484 738.421142578125 

15 self_attn.o_proj
Factorizing 15.self_attn.o_proj...
err 259 20.549673080444336 144.20901489257812
sparsity check:  1.5 bpw
relative err after factorization:  0.18259117551985551 726.4876708984375 

15 self_attn.k_proj
err before 0.05654666824557353
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd646e5c10>
epoch: 0, train err: 0.05546158192737494
epoch: 1, train err: 0.05431825222331099
epoch: 2, train err: 0.053854859579587355
epoch: 3, train err: 0.053696827293606475
Factorizing 15.self_attn.k_proj...
err 259 3.2279319763183594 216.43331909179688
sparsity check:  1.5 bpw
relative err after factorization:  0.1615354710018797 1637.6708984375 

15 mlp.up_proj
Factorizing 15.mlp.up_proj...
err 259 28.710166931152344 192.24124145507812
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1846912528244561 2864.64501953125 

15 mlp.gate_proj
Factorizing 15.mlp.gate_proj...
err 259 21.66497230529785 158.65695190429688
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17138238674503473 2796.486572265625 

15 mlp.down_proj
Factorizing 15.mlp.down_proj...
err 259 42.54566192626953 264.78143310546875
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18562098765319662 2786.533203125 

collecting Q input activations for next block ...
collecting FP target output activations for block 16...
16 self_attn.q_proj
err before 0.06662415961909574
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd687a07d0>
epoch: 0, train err: 0.06373982298828196
epoch: 1, train err: 0.06124752883624751
epoch: 2, train err: 0.06031039789377246
epoch: 3, train err: 0.0600054163223831
Factorizing 16.self_attn.q_proj...
err 259 2.944258213043213 86.05598449707031
sparsity check:  1.5 bpw
relative err after factorization:  0.16297906307326257 1517.7232666015625 

16 self_attn.v_proj
Factorizing 16.self_attn.v_proj...
err 259 76.39616394042969 503.95654296875
sparsity check:  1.5 bpw
relative err after factorization:  0.1867205501076855 829.126220703125 

16 self_attn.o_proj
Factorizing 16.self_attn.o_proj...
err 259 25.321819305419922 165.04464721679688
sparsity check:  1.5 bpw
relative err after factorization:  0.1947564723710328 846.5506591796875 

16 self_attn.k_proj
err before 0.06608927497291006
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64708e10>
epoch: 0, train err: 0.06451746067614295
epoch: 1, train err: 0.06290798717236612
epoch: 2, train err: 0.062303928774781525
epoch: 3, train err: 0.06210416137764696
Factorizing 16.self_attn.k_proj...
err 259 2.8574728965759277 190.19534301757812
sparsity check:  1.5 bpw
relative err after factorization:  0.15920157105939875 1568.775390625 

16 mlp.up_proj
Factorizing 16.mlp.up_proj...
err 259 30.53894805908203 213.91802978515625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18221115897154946 2824.41015625 

16 mlp.gate_proj
Factorizing 16.mlp.gate_proj...
err 259 23.509944915771484 159.7235565185547
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16870176779215515 2792.822509765625 

16 mlp.down_proj
Factorizing 16.mlp.down_proj...
err 259 51.387855529785156 319.9989929199219
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1887755281237825 2834.3447265625 

collecting Q input activations for next block ...
collecting FP target output activations for block 17...
17 self_attn.q_proj
err before 0.07603062898851931
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd687a2250>
epoch: 0, train err: 0.0734014139889041
epoch: 1, train err: 0.07089154518325813
epoch: 2, train err: 0.06989869392418768
epoch: 3, train err: 0.06957096965925302
Factorizing 17.self_attn.q_proj...
err 259 2.6238276958465576 72.54011535644531
sparsity check:  1.5 bpw
relative err after factorization:  0.18020324513939096 1653.2056884765625 

17 self_attn.v_proj
Factorizing 17.self_attn.v_proj...
err 259 90.17340087890625 563.04833984375
sparsity check:  1.5 bpw
relative err after factorization:  0.18924841540959733 836.07177734375 

17 self_attn.o_proj
Factorizing 17.self_attn.o_proj...
err 259 17.317832946777344 109.35453796386719
sparsity check:  1.5 bpw
relative err after factorization:  0.1999262284613579 872.1392211914062 

17 self_attn.k_proj
err before 0.07338676156359725
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64728450>
epoch: 0, train err: 0.0724785934726242
epoch: 1, train err: 0.07110802517854609
epoch: 2, train err: 0.07052270145504735
epoch: 3, train err: 0.07032099120260682
Factorizing 17.self_attn.k_proj...
err 259 2.715501308441162 132.20753479003906
sparsity check:  1.5 bpw
relative err after factorization:  0.17333153683430247 1667.8800048828125 

17 mlp.up_proj
Factorizing 17.mlp.up_proj...
err 259 32.221431732177734 202.01242065429688
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18066873062419533 2777.52978515625 

17 mlp.gate_proj
Factorizing 17.mlp.gate_proj...
err 259 25.415802001953125 165.92422485351562
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1677030565216555 2827.015625 

17 mlp.down_proj
Factorizing 17.mlp.down_proj...
err 259 52.530120849609375 316.8782958984375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18732326880121322 2805.549560546875 

collecting Q input activations for next block ...
collecting FP target output activations for block 18...
18 self_attn.q_proj
err before 0.08626459892548155
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba619e90>
epoch: 0, train err: 0.08359168664901517
epoch: 1, train err: 0.08097418512625154
epoch: 2, train err: 0.07991690721246414
epoch: 3, train err: 0.0795664299512282
Factorizing 18.self_attn.q_proj...
err 259 2.5949597358703613 68.77632141113281
sparsity check:  1.5 bpw
relative err after factorization:  0.18864820343846114 1654.51806640625 

18 self_attn.v_proj
Factorizing 18.self_attn.v_proj...
err 259 113.30521392822266 695.26953125
sparsity check:  1.5 bpw
relative err after factorization:  0.18985650609146965 919.908447265625 

18 self_attn.o_proj
Factorizing 18.self_attn.o_proj...
err 259 17.929454803466797 113.27102661132812
sparsity check:  1.5 bpw
relative err after factorization:  0.20467902859474058 963.7406005859375 

18 self_attn.k_proj
err before 0.08375683239137288
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd6472b290>
epoch: 0, train err: 0.08278253344178665
epoch: 1, train err: 0.08131838963890914
epoch: 2, train err: 0.08068422450742219
epoch: 3, train err: 0.0804647197801387
Factorizing 18.self_attn.k_proj...
err 259 2.8613781929016113 119.28175354003906
sparsity check:  1.5 bpw
relative err after factorization:  0.18399331589715914 1691.4052734375 

18 mlp.up_proj
Factorizing 18.mlp.up_proj...
err 259 34.39551544189453 213.12635803222656
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.182292535263418 2787.12451171875 

18 mlp.gate_proj
Factorizing 18.mlp.gate_proj...
err 259 28.052919387817383 178.49517822265625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1693349072673058 2906.67138671875 

18 mlp.down_proj
Factorizing 18.mlp.down_proj...
err 259 54.148746490478516 334.953125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18875212470288125 2822.134765625 

collecting Q input activations for next block ...
collecting FP target output activations for block 19...
19 self_attn.q_proj
err before 0.09697227306605782
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba619e90>
epoch: 0, train err: 0.09446113536250778
epoch: 1, train err: 0.09183792398835067
epoch: 2, train err: 0.09076504217227921
epoch: 3, train err: 0.09040492060012184
Factorizing 19.self_attn.q_proj...
err 259 2.2243731021881104 60.02752685546875
sparsity check:  1.5 bpw
relative err after factorization:  0.18116729237563806 1550.6182861328125 

19 self_attn.v_proj
Factorizing 19.self_attn.v_proj...
err 259 100.65025329589844 626.5048828125
sparsity check:  1.5 bpw
relative err after factorization:  0.18535278059063706 913.7754516601562 

19 self_attn.o_proj
Factorizing 19.self_attn.o_proj...
err 259 14.689986228942871 90.05284881591797
sparsity check:  1.5 bpw
relative err after factorization:  0.20221282941042468 976.0809326171875 

19 self_attn.k_proj
err before 0.09372995345620438
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64696890>
epoch: 0, train err: 0.09291799171478488
epoch: 1, train err: 0.09147999054403044
epoch: 2, train err: 0.09083288465626538
epoch: 3, train err: 0.09060752723598853
Factorizing 19.self_attn.k_proj...
err 259 2.2432239055633545 104.35311889648438
sparsity check:  1.5 bpw
relative err after factorization:  0.17293254250977586 1543.103759765625 

19 mlp.up_proj
Factorizing 19.mlp.up_proj...
err 259 34.65632629394531 214.55548095703125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18235012618737204 2787.90673828125 

19 mlp.gate_proj
Factorizing 19.mlp.gate_proj...
err 259 29.60885238647461 186.3232879638672
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17034367767119324 2943.911376953125 

19 mlp.down_proj
Factorizing 19.mlp.down_proj...
err 259 52.51873016357422 324.71929931640625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.189406489155416 2844.682373046875 

collecting Q input activations for next block ...
collecting FP target output activations for block 20...
20 self_attn.q_proj
err before 0.10839693178422749
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd672e9f50>
epoch: 0, train err: 0.10601490960107185
epoch: 1, train err: 0.10318901127902791
epoch: 2, train err: 0.10203345268382691
epoch: 3, train err: 0.10164747896487825
Factorizing 20.self_attn.q_proj...
err 259 1.7617428302764893 48.71931076049805
sparsity check:  1.5 bpw
relative err after factorization:  0.17519803909604606 1519.78173828125 

20 self_attn.v_proj
Factorizing 20.self_attn.v_proj...
err 259 93.40567016601562 544.7554931640625
sparsity check:  1.5 bpw
relative err after factorization:  0.18641135307000692 947.5667724609375 

20 self_attn.o_proj
Factorizing 20.self_attn.o_proj...
err 259 15.43022346496582 97.74681854248047
sparsity check:  1.5 bpw
relative err after factorization:  0.2115501675023133 1061.621337890625 

20 self_attn.k_proj
err before 0.1057242413808126
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64701990>
epoch: 0, train err: 0.10460608248831704
epoch: 1, train err: 0.10293350089341402
epoch: 2, train err: 0.10222055535996333
epoch: 3, train err: 0.10197399808384944
Factorizing 20.self_attn.k_proj...
err 259 1.8168926239013672 84.28445434570312
sparsity check:  1.5 bpw
relative err after factorization:  0.1665882678023732 1504.259521484375 

20 mlp.up_proj
Factorizing 20.mlp.up_proj...
err 259 34.29670333862305 209.91622924804688
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1805798775766883 2761.0419921875 

20 mlp.gate_proj
Factorizing 20.mlp.gate_proj...
err 259 29.888046264648438 188.1788787841797
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16964924425145864 2966.27197265625 

20 mlp.down_proj
Factorizing 20.mlp.down_proj...
err 259 55.933753967285156 347.237548828125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1922660550971064 2891.40771484375 

collecting Q input activations for next block ...
collecting FP target output activations for block 21...
21 self_attn.q_proj
err before 0.11713428280199878
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd99e1ccd0>
epoch: 0, train err: 0.11505504947854206
epoch: 1, train err: 0.11243799081421457
epoch: 2, train err: 0.11134682476404123
epoch: 3, train err: 0.11097492516273633
Factorizing 21.self_attn.q_proj...
err 259 1.491750955581665 38.743560791015625
sparsity check:  1.5 bpw
relative err after factorization:  0.17217070838853724 1428.695068359375 

21 self_attn.v_proj
Factorizing 21.self_attn.v_proj...
err 259 105.53331756591797 629.4204711914062
sparsity check:  1.5 bpw
relative err after factorization:  0.18403392860353082 994.7122802734375 

21 self_attn.o_proj
Factorizing 21.self_attn.o_proj...
err 259 12.881099700927734 75.04531860351562
sparsity check:  1.5 bpw
relative err after factorization:  0.2044924178448696 1073.3701171875 

21 self_attn.k_proj
err before 0.11392475425964221
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64bc4ad0>
epoch: 0, train err: 0.11324846127536148
epoch: 1, train err: 0.11175642881426029
epoch: 2, train err: 0.11107373831328005
epoch: 3, train err: 0.11083297015284188
Factorizing 21.self_attn.k_proj...
err 259 1.6651334762573242 74.10480499267578
sparsity check:  1.5 bpw
relative err after factorization:  0.16870658967525326 1436.40283203125 

21 mlp.up_proj
Factorizing 21.mlp.up_proj...
err 259 33.40276336669922 204.17518615722656
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1801698024987939 2744.775390625 

21 mlp.gate_proj
Factorizing 21.mlp.gate_proj...
err 259 29.810482025146484 184.1733856201172
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16953843122701842 3001.6474609375 

21 mlp.down_proj
Factorizing 21.mlp.down_proj...
err 259 50.354740142822266 307.98046875
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19128634322714858 2875.62646484375 

collecting Q input activations for next block ...
collecting FP target output activations for block 22...
22 self_attn.q_proj
err before 0.12432969038491137
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd68887850>
epoch: 0, train err: 0.1223207910079509
epoch: 1, train err: 0.11973157402826473
epoch: 2, train err: 0.11863218026701361
epoch: 3, train err: 0.11825664568459615
Factorizing 22.self_attn.q_proj...
err 259 1.9772589206695557 48.09886169433594
sparsity check:  1.5 bpw
relative err after factorization:  0.17480938319773018 1512.49755859375 

22 self_attn.v_proj
Factorizing 22.self_attn.v_proj...
err 259 95.70894622802734 563.8858642578125
sparsity check:  1.5 bpw
relative err after factorization:  0.18970092602562721 1032.27685546875 

22 self_attn.o_proj
Factorizing 22.self_attn.o_proj...
err 259 13.703324317932129 81.66859436035156
sparsity check:  1.5 bpw
relative err after factorization:  0.20631324997214284 1087.59228515625 

22 self_attn.k_proj
err before 0.12184947601053864
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64bc7e10>
epoch: 0, train err: 0.1210224179958459
epoch: 1, train err: 0.11944183116429485
epoch: 2, train err: 0.11874520950368606
epoch: 3, train err: 0.11850128075457178
Factorizing 22.self_attn.k_proj...
err 259 2.2139220237731934 87.58163452148438
sparsity check:  1.5 bpw
relative err after factorization:  0.17479230518618627 1556.0850830078125 

22 mlp.up_proj
Factorizing 22.mlp.up_proj...
err 259 32.7427978515625 196.96670532226562
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1795200539839249 2729.974609375 

22 mlp.gate_proj
Factorizing 22.mlp.gate_proj...
err 259 29.530475616455078 181.26080322265625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.169465282781029 3031.156005859375 

22 mlp.down_proj
Factorizing 22.mlp.down_proj...
err 259 48.78766632080078 295.63720703125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19010380625084633 2858.5068359375 

collecting Q input activations for next block ...
collecting FP target output activations for block 23...
23 self_attn.q_proj
err before 0.12749573661130853
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd646a3350>
epoch: 0, train err: 0.12587919863290153
epoch: 1, train err: 0.12354203622089699
epoch: 2, train err: 0.12254170002415776
epoch: 3, train err: 0.12220084184082225
Factorizing 23.self_attn.q_proj...
err 259 1.9810113906860352 42.261634826660156
sparsity check:  1.5 bpw
relative err after factorization:  0.18695021990059793 1595.6234130859375 

23 self_attn.v_proj
Factorizing 23.self_attn.v_proj...
err 259 116.15557861328125 679.3007202148438
sparsity check:  1.5 bpw
relative err after factorization:  0.18810404798411226 1121.984619140625 

23 self_attn.o_proj
Factorizing 23.self_attn.o_proj...
err 259 11.47098159790039 66.67359161376953
sparsity check:  1.5 bpw
relative err after factorization:  0.20281818297028878 1179.9630126953125 

23 self_attn.k_proj
err before 0.12478352719335817
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64beaf50>
epoch: 0, train err: 0.12419254134874791
epoch: 1, train err: 0.12278808222617954
epoch: 2, train err: 0.12214888882590458
epoch: 3, train err: 0.12192349880933762
Factorizing 23.self_attn.k_proj...
err 259 2.3032217025756836 73.63175964355469
sparsity check:  1.5 bpw
relative err after factorization:  0.18329564416608607 1595.23291015625 

23 mlp.up_proj
Factorizing 23.mlp.up_proj...
err 259 32.41707992553711 193.0573272705078
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18033584527104735 2769.161865234375 

23 mlp.gate_proj
Factorizing 23.mlp.gate_proj...
err 259 29.934165954589844 182.47372436523438
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1712091312061194 3059.923828125 

23 mlp.down_proj
Factorizing 23.mlp.down_proj...
err 259 43.15806579589844 262.93603515625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1880535661068068 2861.12353515625 

collecting Q input activations for next block ...
collecting FP target output activations for block 24...
24 self_attn.q_proj
err before 0.129488606704399
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba6125d0>
epoch: 0, train err: 0.12809136387659237
epoch: 1, train err: 0.12582918084808625
epoch: 2, train err: 0.1248627522145398
epoch: 3, train err: 0.1245316487911623
Factorizing 24.self_attn.q_proj...
err 259 1.656506061553955 43.043724060058594
sparsity check:  1.5 bpw
relative err after factorization:  0.16667308625197358 1343.79931640625 

24 self_attn.v_proj
Factorizing 24.self_attn.v_proj...
err 259 84.38622283935547 496.4586181640625
sparsity check:  1.5 bpw
relative err after factorization:  0.18588534425672368 1088.5679931640625 

24 self_attn.o_proj
Factorizing 24.self_attn.o_proj...
err 259 11.342567443847656 70.173583984375
sparsity check:  1.5 bpw
relative err after factorization:  0.21112082264119922 1193.7567138671875 

24 self_attn.k_proj
err before 0.1270799141493626
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64c09d10>
epoch: 0, train err: 0.12657169887097552
epoch: 1, train err: 0.12526602746220306
epoch: 2, train err: 0.12466385992593132
epoch: 3, train err: 0.1244497692969162
Factorizing 24.self_attn.k_proj...
err 259 1.7739924192428589 85.67018127441406
sparsity check:  1.5 bpw
relative err after factorization:  0.1665000727840251 1361.049560546875 

24 mlp.up_proj
Factorizing 24.mlp.up_proj...
err 259 31.52530288696289 186.20997619628906
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1810520350625909 2798.190673828125 

24 mlp.gate_proj
Factorizing 24.mlp.gate_proj...
err 259 29.520198822021484 178.697021484375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17212144261425172 3091.58349609375 

24 mlp.down_proj
Factorizing 24.mlp.down_proj...
err 259 38.71266174316406 238.0902099609375
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18653140603158444 2859.202392578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 25...
25 self_attn.q_proj
err before 0.12774561982951127
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba63c850>
epoch: 0, train err: 0.1264165757165756
epoch: 1, train err: 0.12445757241221145
epoch: 2, train err: 0.12360747961793095
epoch: 3, train err: 0.12331736457417719
Factorizing 25.self_attn.q_proj...
err 259 1.8346973657608032 39.686336517333984
sparsity check:  1.5 bpw
relative err after factorization:  0.1810625909925825 1485.3515625 

25 self_attn.v_proj
Factorizing 25.self_attn.v_proj...
err 259 105.18375396728516 607.4812622070312
sparsity check:  1.5 bpw
relative err after factorization:  0.18879734382780908 1190.51318359375 

25 self_attn.o_proj
Factorizing 25.self_attn.o_proj...
err 259 8.277929306030273 49.64198303222656
sparsity check:  1.5 bpw
relative err after factorization:  0.20306850202839458 1244.4945068359375 

25 self_attn.k_proj
err before 0.12514908373123035
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64c28c90>
epoch: 0, train err: 0.12471858458593488
epoch: 1, train err: 0.12354620033875108
epoch: 2, train err: 0.12299906575935893
epoch: 3, train err: 0.12280422184267081
Factorizing 25.self_attn.k_proj...
err 259 2.26517391204834 68.16539001464844
sparsity check:  1.5 bpw
relative err after factorization:  0.1825717368052878 1512.9334716796875 

25 mlp.up_proj
Factorizing 25.mlp.up_proj...
err 259 31.684640884399414 188.28822326660156
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1827954678021392 2848.6181640625 

25 mlp.gate_proj
Factorizing 25.mlp.gate_proj...
err 259 30.10378646850586 182.81448364257812
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17387023263304907 3133.275390625 

25 mlp.down_proj
Factorizing 25.mlp.down_proj...
err 259 34.168758392333984 215.8990478515625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1859618780940877 2877.64111328125 

collecting Q input activations for next block ...
collecting FP target output activations for block 26...
26 self_attn.q_proj
err before 0.1261880352103617
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba63c3d0>
epoch: 0, train err: 0.12488011585082859
epoch: 1, train err: 0.1229083092766814
epoch: 2, train err: 0.12206078186864033
epoch: 3, train err: 0.12177242175675929
Factorizing 26.self_attn.q_proj...
err 259 1.8362315893173218 43.672603607177734
sparsity check:  1.5 bpw
relative err after factorization:  0.17691497321736854 1404.557861328125 

26 self_attn.v_proj
Factorizing 26.self_attn.v_proj...
err 259 79.98176574707031 507.60748291015625
sparsity check:  1.5 bpw
relative err after factorization:  0.1884163110628319 1226.18115234375 

26 self_attn.o_proj
Factorizing 26.self_attn.o_proj...
err 259 9.926427841186523 65.85713958740234
sparsity check:  1.5 bpw
relative err after factorization:  0.22245430984148434 1428.1708984375 

26 self_attn.k_proj
err before 0.12444776843767613
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b30390>
epoch: 0, train err: 0.12391226235195063
epoch: 1, train err: 0.12276779368403368
epoch: 2, train err: 0.12223960491246544
epoch: 3, train err: 0.12205171029199846
Factorizing 26.self_attn.k_proj...
err 259 2.0984320640563965 78.20976257324219
sparsity check:  1.5 bpw
relative err after factorization:  0.17470246139398318 1412.6649169921875 

26 mlp.up_proj
Factorizing 26.mlp.up_proj...
err 259 31.51819610595703 188.00445556640625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18471937059250762 2898.472412109375 

26 mlp.gate_proj
Factorizing 26.mlp.gate_proj...
err 259 30.156661987304688 184.53408813476562
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17452413206489106 3162.65576171875 

26 mlp.down_proj
Factorizing 26.mlp.down_proj...
err 259 31.33765983581543 200.0298614501953
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18738222925402742 2916.455078125 

collecting Q input activations for next block ...
collecting FP target output activations for block 27...
27 self_attn.q_proj
err before 0.12188595728366636
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd99d20310>
epoch: 0, train err: 0.12058574421098456
epoch: 1, train err: 0.11889985809102654
epoch: 2, train err: 0.11816763124079444
epoch: 3, train err: 0.11791749406256713
Factorizing 27.self_attn.q_proj...
err 259 1.9073865413665771 31.255130767822266
sparsity check:  1.5 bpw
relative err after factorization:  0.19292649100851808 1648.668212890625 

27 self_attn.v_proj
Factorizing 27.self_attn.v_proj...
err 259 82.09488677978516 471.5165710449219
sparsity check:  1.5 bpw
relative err after factorization:  0.19333871894523966 1262.428955078125 

27 self_attn.o_proj
Factorizing 27.self_attn.o_proj...
err 259 7.792007923126221 50.61701202392578
sparsity check:  1.5 bpw
relative err after factorization:  0.21249130867959562 1382.251708984375 

27 self_attn.k_proj
err before 0.11978561038267799
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b33410>
epoch: 0, train err: 0.11931202784762718
epoch: 1, train err: 0.11828541147406213
epoch: 2, train err: 0.11780397713300772
epoch: 3, train err: 0.11763337574666366
Factorizing 27.self_attn.k_proj...
err 259 2.4360055923461914 52.79262161254883
sparsity check:  1.5 bpw
relative err after factorization:  0.1940222621361502 1695.2373046875 

27 mlp.up_proj
Factorizing 27.mlp.up_proj...
err 259 32.13850021362305 197.82911682128906
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1877140516487385 2974.502197265625 

27 mlp.gate_proj
Factorizing 27.mlp.gate_proj...
err 259 30.91823959350586 195.5358428955078
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17642520383569466 3202.9130859375 

27 mlp.down_proj
Factorizing 27.mlp.down_proj...
err 259 29.282928466796875 192.7353973388672
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18969391119782708 2983.170166015625 

collecting Q input activations for next block ...
collecting FP target output activations for block 28...
28 self_attn.q_proj
err before 0.11935604651807807
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba77e890>
epoch: 0, train err: 0.11790045103407465
epoch: 1, train err: 0.11613454940379597
epoch: 2, train err: 0.11538988255779259
epoch: 3, train err: 0.11513632509741001
Factorizing 28.self_attn.q_proj...
err 259 1.7926149368286133 36.5986442565918
sparsity check:  1.5 bpw
relative err after factorization:  0.19584157798558302 1579.815185546875 

28 self_attn.v_proj
Factorizing 28.self_attn.v_proj...
err 259 73.99950408935547 443.5455627441406
sparsity check:  1.5 bpw
relative err after factorization:  0.19443424114269112 1351.5067138671875 

28 self_attn.o_proj
Factorizing 28.self_attn.o_proj...
err 259 7.621079921722412 56.093692779541016
sparsity check:  1.5 bpw
relative err after factorization:  0.21793180819936023 1501.10791015625 

28 self_attn.k_proj
err before 0.11716591083677486
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b4e610>
epoch: 0, train err: 0.11665194897796027
epoch: 1, train err: 0.11562685150420293
epoch: 2, train err: 0.11514721470302902
epoch: 3, train err: 0.11497792036971077
Factorizing 28.self_attn.k_proj...
err 259 2.3112545013427734 77.81108093261719
sparsity check:  1.5 bpw
relative err after factorization:  0.20367294802094996 1686.290283203125 

28 mlp.up_proj
Factorizing 28.mlp.up_proj...
err 259 30.91329002380371 206.76699829101562
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19181034873756722 3097.42431640625 

28 mlp.gate_proj
Factorizing 28.mlp.gate_proj...
err 259 29.135272979736328 193.79530334472656
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.177337361947464 3188.83056640625 

28 mlp.down_proj
Factorizing 28.mlp.down_proj...
err 259 29.586380004882812 204.0160369873047
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19675062747219257 3130.86083984375 

collecting Q input activations for next block ...
collecting FP target output activations for block 29...
29 self_attn.q_proj
err before 0.11953425721731037
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba7a5c90>
epoch: 0, train err: 0.11799382569734007
epoch: 1, train err: 0.11619482756941579
epoch: 2, train err: 0.11544961982872337
epoch: 3, train err: 0.11519168032100424
Factorizing 29.self_attn.q_proj...
err 259 1.3530638217926025 25.072364807128906
sparsity check:  1.5 bpw
relative err after factorization:  0.17867649202661112 1367.4859619140625 

29 self_attn.v_proj
Factorizing 29.self_attn.v_proj...
err 259 52.256160736083984 308.00360107421875
sparsity check:  1.5 bpw
relative err after factorization:  0.19151106540443788 1342.110107421875 

29 self_attn.o_proj
Factorizing 29.self_attn.o_proj...
err 259 6.9507598876953125 44.3302001953125
sparsity check:  1.5 bpw
relative err after factorization:  0.20748917035730552 1463.615234375 

29 self_attn.k_proj
err before 0.11692675773520023
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b69810>
epoch: 0, train err: 0.1164434542588424
epoch: 1, train err: 0.1154529495688621
epoch: 2, train err: 0.11497878402587958
epoch: 3, train err: 0.11480833080713637
Factorizing 29.self_attn.k_proj...
err 259 1.5487794876098633 49.812984466552734
sparsity check:  1.5 bpw
relative err after factorization:  0.1800432386880999 1404.3736572265625 

29 mlp.up_proj
Factorizing 29.mlp.up_proj...
err 259 30.911062240600586 245.4112548828125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1970896332206402 3240.484619140625 

29 mlp.gate_proj
Factorizing 29.mlp.gate_proj...
err 259 29.22905731201172 269.11895751953125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18132890973183324 3276.763916015625 

29 mlp.down_proj
Factorizing 29.mlp.down_proj...
err 259 30.54942512512207 221.81857299804688
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20403853685931642 3269.146484375 

collecting Q input activations for next block ...
collecting FP target output activations for block 30...
30 self_attn.q_proj
err before 0.13127138430718333
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efdba7a5a10>
epoch: 0, train err: 0.12821761166560464
epoch: 1, train err: 0.1257642133277841
epoch: 2, train err: 0.12489179556723684
epoch: 3, train err: 0.12458793193218298
Factorizing 30.self_attn.q_proj...
err 259 1.3993607759475708 20.424236297607422
sparsity check:  1.5 bpw
relative err after factorization:  0.18651742212383338 1445.8134765625 

30 self_attn.v_proj
Factorizing 30.self_attn.v_proj...
err 259 60.75249481201172 353.9527587890625
sparsity check:  1.5 bpw
relative err after factorization:  0.19691753303438692 1446.0849609375 

30 self_attn.o_proj
Factorizing 30.self_attn.o_proj...
err 259 6.883716583251953 49.631614685058594
sparsity check:  1.5 bpw
relative err after factorization:  0.22275130039350052 1651.1846923828125 

30 self_attn.k_proj
err before 0.12666148005519062
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b84750>
epoch: 0, train err: 0.1258613042300567
epoch: 1, train err: 0.12470660658436827
epoch: 2, train err: 0.12418329459615052
epoch: 3, train err: 0.12398268669494428
Factorizing 30.self_attn.k_proj...
err 259 1.7112510204315186 32.375038146972656
sparsity check:  1.5 bpw
relative err after factorization:  0.18850354721771384 1503.5234375 

30 mlp.up_proj
Factorizing 30.mlp.up_proj...
err 259 33.717987060546875 345.1402587890625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2026154332482898 3445.770263671875 

30 mlp.gate_proj
Factorizing 30.mlp.gate_proj...
err 259 30.531814575195312 269.4164733886719
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18020644887819473 3409.38916015625 

30 mlp.down_proj
Factorizing 30.mlp.down_proj...
err 259 36.93464660644531 931.35595703125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.21724238989776082 3440.650390625 

collecting Q input activations for next block ...
collecting FP target output activations for block 31...
31 self_attn.q_proj
err before 0.32799887243891135
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd66ae6090>
epoch: 0, train err: 0.27028337749652565
epoch: 1, train err: 0.2562728126067668
epoch: 2, train err: 0.25324162730248645
epoch: 3, train err: 0.25227865064516664
Factorizing 31.self_attn.q_proj...
err 259 0.9714618921279907 16.558425903320312
sparsity check:  1.5 bpw
relative err after factorization:  0.17564367705823533 1394.622802734375 

31 self_attn.v_proj
Factorizing 31.self_attn.v_proj...
err 259 19.875003814697266 123.44058227539062
sparsity check:  1.5 bpw
relative err after factorization:  0.1959462054493549 1195.7581787109375 

31 self_attn.o_proj
Factorizing 31.self_attn.o_proj...
err 259 5.737123966217041 51.08032989501953
sparsity check:  1.5 bpw
relative err after factorization:  0.20789650213364905 1293.84814453125 

31 self_attn.k_proj
err before 0.2591655259602703
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.float32, torch.float32, torch.float32, torch.float32]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd64b87a50>
epoch: 0, train err: 0.2543154156883247
epoch: 1, train err: 0.2507080092909746
epoch: 2, train err: 0.24922407779376954
epoch: 3, train err: 0.24865569086978212
Factorizing 31.self_attn.k_proj...
err 259 0.9949042797088623 19.600040435791016
sparsity check:  1.5 bpw
relative err after factorization:  0.1739566599615453 1484.5084228515625 

31 mlp.up_proj
Factorizing 31.mlp.up_proj...
err 259 38.4074821472168 615.08251953125
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.21090430673853416 3877.681640625 

31 mlp.gate_proj
Factorizing 31.mlp.gate_proj...
err 259 32.09539794921875 294.6141357421875
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1828846283808531 3779.134033203125 

31 mlp.down_proj
Factorizing 31.mlp.down_proj...
err 259 64.88835906982422 1436.420166015625
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.23430708656739105 3697.196044921875 

total time 36572.21702170372
saving model...
Model's save dict has been saved!
Testing PPL...

Start evaluation... 83 339968
Got PPL 10.132203069569947
