{'gpu': '0', 'target_bits': 1.5, 'model_name': 'meta-llama/Llama-2-7b-hf', 'model_path': '/mnt/disk2-part1/pretrained-models-pytorch/llm/llama2_7b', 'data_path': '/mnt/disk2-part1/datasets/llm', 'train_dataset': 'redpajamas', 'seqlen': 4096, 'only_full_ft': False, 'eval': False, 'one_cycle_lr': False, 'n_calib_data': 256, 'n_epochs': 4, 'norm_order': 2.0, 'n_grad_accumu': 8, 'lr': 3e-05, 'weight_decay': 0.0001, 'train_scaling_vectors': False, 'lr_s': 0.0001, 'epoch_s': 2, 'admm_reg': 0.03, 'alter_opt_outer_iters': 260, 'alter_opt_inner_iters': 3, 'save_dir': '/projects/p487-24-1/dbf_rep', 'is_save_ckpt': True}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.45s/it]
ms 4096
total params:  6738415616 
linear layers params:  6476267520
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
eval_loader torch.Size([1, 341469])
Collecting i_norm and o_norm for linear layers...
  0%|                                                                                                                                                                | 0/256 [00:00<?, ?it/s]0 tensor(0.6680, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  0%|▌                                                                                                                                                       | 1/256 [00:01<07:38,  1.80s/it]1 tensor(1.9688, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  1%|█▏                                                                                                                                                      | 2/256 [00:03<06:13,  1.47s/it]2 tensor(1.5703, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  1%|█▊                                                                                                                                                      | 3/256 [00:04<05:47,  1.37s/it]3 tensor(1.8828, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  2%|██▍                                                                                                                                                     | 4/256 [00:05<05:34,  1.33s/it]4 tensor(1.6328, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  2%|██▉                                                                                                                                                     | 5/256 [00:06<05:27,  1.30s/it]5 tensor(2.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  2%|███▌                                                                                                                                                    | 6/256 [00:08<05:22,  1.29s/it]6 tensor(1.9922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  3%|████▏                                                                                                                                                   | 7/256 [00:09<05:18,  1.28s/it]7 tensor(1.2422, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  3%|████▊                                                                                                                                                   | 8/256 [00:10<05:15,  1.27s/it]8 tensor(2.4219, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  4%|█████▎                                                                                                                                                  | 9/256 [00:11<05:13,  1.27s/it]9 tensor(1.6797, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  4%|█████▉                                                                                                                                                 | 10/256 [00:13<05:11,  1.27s/it]10 tensor(1.9453, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  4%|██████▍                                                                                                                                                | 11/256 [00:14<05:09,  1.27s/it]11 tensor(2.3125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  5%|███████                                                                                                                                                | 12/256 [00:15<05:08,  1.26s/it]12 tensor(1.3906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  5%|███████▋                                                                                                                                               | 13/256 [00:16<05:06,  1.26s/it]13 tensor(1.9844, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  5%|████████▎                                                                                                                                              | 14/256 [00:18<05:05,  1.26s/it]14 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  6%|████████▊                                                                                                                                              | 15/256 [00:19<05:04,  1.26s/it]15 tensor(1.7734, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  6%|█████████▍                                                                                                                                             | 16/256 [00:20<05:02,  1.26s/it]16 tensor(1.8594, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  7%|██████████                                                                                                                                             | 17/256 [00:21<05:01,  1.26s/it]17 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  7%|██████████▌                                                                                                                                            | 18/256 [00:23<05:00,  1.26s/it]18 tensor(1.6250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  7%|███████████▏                                                                                                                                           | 19/256 [00:24<04:58,  1.26s/it]19 tensor(2.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  8%|███████████▊                                                                                                                                           | 20/256 [00:25<04:57,  1.26s/it]20 tensor(1.3359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  8%|████████████▍                                                                                                                                          | 21/256 [00:26<04:56,  1.26s/it]21 tensor(1.7578, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  9%|████████████▉                                                                                                                                          | 22/256 [00:28<04:55,  1.26s/it]22 tensor(2.2656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  9%|█████████████▌                                                                                                                                         | 23/256 [00:29<04:53,  1.26s/it]23 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
  9%|██████████████▏                                                                                                                                        | 24/256 [00:30<04:52,  1.26s/it]24 tensor(1.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 10%|██████████████▋                                                                                                                                        | 25/256 [00:32<04:51,  1.26s/it]25 tensor(0.6250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 10%|███████████████▎                                                                                                                                       | 26/256 [00:33<04:50,  1.26s/it]26 tensor(2.2969, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 11%|███████████████▉                                                                                                                                       | 27/256 [00:34<04:48,  1.26s/it]27 tensor(1.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 11%|████████████████▌                                                                                                                                      | 28/256 [00:35<04:47,  1.26s/it]28 tensor(1.9531, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 11%|█████████████████                                                                                                                                      | 29/256 [00:37<04:46,  1.26s/it]29 tensor(2.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 12%|█████████████████▋                                                                                                                                     | 30/256 [00:38<04:45,  1.26s/it]30 tensor(1.6094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 12%|██████████████████▎                                                                                                                                    | 31/256 [00:39<04:43,  1.26s/it]31 tensor(2.2031, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 12%|██████████████████▉                                                                                                                                    | 32/256 [00:40<04:42,  1.26s/it]32 tensor(1.6719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 13%|███████████████████▍                                                                                                                                   | 33/256 [00:42<04:41,  1.26s/it]33 tensor(0.6094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 13%|████████████████████                                                                                                                                   | 34/256 [00:43<04:40,  1.26s/it]34 tensor(0.9727, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 14%|████████████████████▋                                                                                                                                  | 35/256 [00:44<04:38,  1.26s/it]35 tensor(2.0156, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 14%|█████████████████████▏                                                                                                                                 | 36/256 [00:45<04:37,  1.26s/it]36 tensor(1.8125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 14%|█████████████████████▊                                                                                                                                 | 37/256 [00:47<04:36,  1.26s/it]37 tensor(2.2969, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 15%|██████████████████████▍                                                                                                                                | 38/256 [00:48<04:34,  1.26s/it]38 tensor(1.7656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 15%|███████████████████████                                                                                                                                | 39/256 [00:49<04:33,  1.26s/it]39 tensor(1.8516, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 16%|███████████████████████▌                                                                                                                               | 40/256 [00:50<04:32,  1.26s/it]40 tensor(0.8633, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 16%|████████████████████████▏                                                                                                                              | 41/256 [00:52<04:31,  1.26s/it]41 tensor(0.1030, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 16%|████████████████████████▊                                                                                                                              | 42/256 [00:53<04:30,  1.26s/it]42 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 17%|█████████████████████████▎                                                                                                                             | 43/256 [00:54<04:28,  1.26s/it]43 tensor(1.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 17%|█████████████████████████▉                                                                                                                             | 44/256 [00:56<04:27,  1.26s/it]44 tensor(0.6992, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 18%|██████████████████████████▌                                                                                                                            | 45/256 [00:57<04:26,  1.26s/it]45 tensor(1.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 18%|███████████████████████████▏                                                                                                                           | 46/256 [00:58<04:24,  1.26s/it]46 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 18%|███████████████████████████▋                                                                                                                           | 47/256 [00:59<04:23,  1.26s/it]47 tensor(1.8047, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 19%|████████████████████████████▎                                                                                                                          | 48/256 [01:01<04:22,  1.26s/it]48 tensor(1.5234, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 19%|████████████████████████████▉                                                                                                                          | 49/256 [01:02<04:21,  1.26s/it]49 tensor(1.2188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 20%|█████████████████████████████▍                                                                                                                         | 50/256 [01:03<04:19,  1.26s/it]50 tensor(1.5078, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 20%|██████████████████████████████                                                                                                                         | 51/256 [01:04<04:18,  1.26s/it]51 tensor(1.5703, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 20%|██████████████████████████████▋                                                                                                                        | 52/256 [01:06<04:17,  1.26s/it]52 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 21%|███████████████████████████████▎                                                                                                                       | 53/256 [01:07<04:16,  1.26s/it]53 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 21%|███████████████████████████████▊                                                                                                                       | 54/256 [01:08<04:14,  1.26s/it]54 tensor(1.8359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 21%|████████████████████████████████▍                                                                                                                      | 55/256 [01:09<04:13,  1.26s/it]55 tensor(1.5000, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 22%|█████████████████████████████████                                                                                                                      | 56/256 [01:11<04:12,  1.26s/it]56 tensor(1.9297, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 22%|█████████████████████████████████▌                                                                                                                     | 57/256 [01:12<04:11,  1.26s/it]57 tensor(1.4297, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 23%|██████████████████████████████████▏                                                                                                                    | 58/256 [01:13<04:09,  1.26s/it]58 tensor(1.2734, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 23%|██████████████████████████████████▊                                                                                                                    | 59/256 [01:14<04:08,  1.26s/it]59 tensor(1.6484, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 23%|███████████████████████████████████▍                                                                                                                   | 60/256 [01:16<04:07,  1.26s/it]60 tensor(2.1719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 24%|███████████████████████████████████▉                                                                                                                   | 61/256 [01:17<04:06,  1.26s/it]61 tensor(1.9766, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 24%|████████████████████████████████████▌                                                                                                                  | 62/256 [01:18<04:04,  1.26s/it]62 tensor(1.6562, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 25%|█████████████████████████████████████▏                                                                                                                 | 63/256 [01:19<04:03,  1.26s/it]63 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 25%|█████████████████████████████████████▊                                                                                                                 | 64/256 [01:21<04:02,  1.26s/it]64 tensor(1.7891, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 25%|██████████████████████████████████████▎                                                                                                                | 65/256 [01:22<04:00,  1.26s/it]65 tensor(1.8359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 26%|██████████████████████████████████████▉                                                                                                                | 66/256 [01:23<03:59,  1.26s/it]66 tensor(1.9297, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 26%|███████████████████████████████████████▌                                                                                                               | 67/256 [01:25<03:58,  1.26s/it]67 tensor(1.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 27%|████████████████████████████████████████                                                                                                               | 68/256 [01:26<03:57,  1.26s/it]68 tensor(1.3203, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 27%|████████████████████████████████████████▋                                                                                                              | 69/256 [01:27<03:56,  1.26s/it]69 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 27%|█████████████████████████████████████████▎                                                                                                             | 70/256 [01:28<03:54,  1.26s/it]70 tensor(1.6953, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 28%|█████████████████████████████████████████▉                                                                                                             | 71/256 [01:30<03:53,  1.26s/it]71 tensor(2.4219, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 28%|██████████████████████████████████████████▍                                                                                                            | 72/256 [01:31<03:52,  1.26s/it]72 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 29%|███████████████████████████████████████████                                                                                                            | 73/256 [01:32<03:50,  1.26s/it]73 tensor(1.8047, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 29%|███████████████████████████████████████████▋                                                                                                           | 74/256 [01:33<03:49,  1.26s/it]74 tensor(1.4297, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 29%|████████████████████████████████████████████▏                                                                                                          | 75/256 [01:35<03:48,  1.26s/it]75 tensor(1.2656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 30%|████████████████████████████████████████████▊                                                                                                          | 76/256 [01:36<03:47,  1.26s/it]76 tensor(1.2422, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 30%|█████████████████████████████████████████████▍                                                                                                         | 77/256 [01:37<03:45,  1.26s/it]77 tensor(1.9766, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 30%|██████████████████████████████████████████████                                                                                                         | 78/256 [01:38<03:44,  1.26s/it]78 tensor(1.7344, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 31%|██████████████████████████████████████████████▌                                                                                                        | 79/256 [01:40<03:43,  1.26s/it]79 tensor(1.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 31%|███████████████████████████████████████████████▏                                                                                                       | 80/256 [01:41<03:42,  1.26s/it]80 tensor(1.8125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 32%|███████████████████████████████████████████████▊                                                                                                       | 81/256 [01:42<03:41,  1.26s/it]81 tensor(1.6328, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 32%|████████████████████████████████████████████████▎                                                                                                      | 82/256 [01:43<03:39,  1.26s/it]82 tensor(1.8359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 32%|████████████████████████████████████████████████▉                                                                                                      | 83/256 [01:45<03:38,  1.26s/it]83 tensor(1.7578, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 33%|█████████████████████████████████████████████████▌                                                                                                     | 84/256 [01:46<03:37,  1.26s/it]84 tensor(0.8789, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 33%|██████████████████████████████████████████████████▏                                                                                                    | 85/256 [01:47<03:35,  1.26s/it]85 tensor(2.1719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 34%|██████████████████████████████████████████████████▋                                                                                                    | 86/256 [01:49<03:34,  1.26s/it]86 tensor(1.7656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 34%|███████████████████████████████████████████████████▎                                                                                                   | 87/256 [01:50<03:33,  1.26s/it]87 tensor(1.4375, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 34%|███████████████████████████████████████████████████▉                                                                                                   | 88/256 [01:51<03:32,  1.26s/it]88 tensor(2.2969, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 35%|████████████████████████████████████████████████████▍                                                                                                  | 89/256 [01:52<03:30,  1.26s/it]89 tensor(1.5938, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 35%|█████████████████████████████████████████████████████                                                                                                  | 90/256 [01:54<03:29,  1.26s/it]90 tensor(1.3281, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 36%|█████████████████████████████████████████████████████▋                                                                                                 | 91/256 [01:55<03:28,  1.26s/it]91 tensor(2.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 36%|██████████████████████████████████████████████████████▎                                                                                                | 92/256 [01:56<03:27,  1.26s/it]92 tensor(1.5781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 36%|██████████████████████████████████████████████████████▊                                                                                                | 93/256 [01:57<03:26,  1.26s/it]93 tensor(2.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 37%|███████████████████████████████████████████████████████▍                                                                                               | 94/256 [01:59<03:24,  1.27s/it]94 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 37%|████████████████████████████████████████████████████████                                                                                               | 95/256 [02:00<03:23,  1.27s/it]95 tensor(1.8047, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 38%|████████████████████████████████████████████████████████▋                                                                                              | 96/256 [02:01<03:22,  1.27s/it]96 tensor(1.5312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 38%|█████████████████████████████████████████████████████████▏                                                                                             | 97/256 [02:02<03:21,  1.27s/it]97 tensor(1.8281, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 38%|█████████████████████████████████████████████████████████▊                                                                                             | 98/256 [02:04<03:20,  1.27s/it]98 tensor(1.3906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 39%|██████████████████████████████████████████████████████████▍                                                                                            | 99/256 [02:05<03:18,  1.27s/it]99 tensor(1.9062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 39%|██████████████████████████████████████████████████████████▌                                                                                           | 100/256 [02:06<03:17,  1.27s/it]100 tensor(2.1719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 39%|███████████████████████████████████████████████████████████▏                                                                                          | 101/256 [02:08<03:16,  1.27s/it]101 tensor(1.7969, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 40%|███████████████████████████████████████████████████████████▊                                                                                          | 102/256 [02:09<03:14,  1.27s/it]102 tensor(1.5703, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 40%|████████████████████████████████████████████████████████████▎                                                                                         | 103/256 [02:10<03:13,  1.27s/it]103 tensor(2.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 41%|████████████████████████████████████████████████████████████▉                                                                                         | 104/256 [02:11<03:12,  1.27s/it]104 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 41%|█████████████████████████████████████████████████████████████▌                                                                                        | 105/256 [02:13<03:11,  1.27s/it]105 tensor(1.1953, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 41%|██████████████████████████████████████████████████████████████                                                                                        | 106/256 [02:14<03:09,  1.27s/it]106 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 42%|██████████████████████████████████████████████████████████████▋                                                                                       | 107/256 [02:15<03:08,  1.27s/it]107 tensor(0.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 42%|███████████████████████████████████████████████████████████████▎                                                                                      | 108/256 [02:16<03:07,  1.27s/it]108 tensor(1.9141, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 43%|███████████████████████████████████████████████████████████████▊                                                                                      | 109/256 [02:18<03:06,  1.27s/it]109 tensor(1.5781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 43%|████████████████████████████████████████████████████████████████▍                                                                                     | 110/256 [02:19<03:04,  1.27s/it]110 tensor(2.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 43%|█████████████████████████████████████████████████████████████████                                                                                     | 111/256 [02:20<03:03,  1.27s/it]111 tensor(0.9766, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 44%|█████████████████████████████████████████████████████████████████▋                                                                                    | 112/256 [02:21<03:02,  1.27s/it]112 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 44%|██████████████████████████████████████████████████████████████████▏                                                                                   | 113/256 [02:23<03:01,  1.27s/it]113 tensor(1.9844, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 45%|██████████████████████████████████████████████████████████████████▊                                                                                   | 114/256 [02:24<02:59,  1.27s/it]114 tensor(2.1250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 45%|███████████████████████████████████████████████████████████████████▍                                                                                  | 115/256 [02:25<02:58,  1.27s/it]115 tensor(1.3438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 45%|███████████████████████████████████████████████████████████████████▉                                                                                  | 116/256 [02:26<02:57,  1.27s/it]116 tensor(1.8438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 46%|████████████████████████████████████████████████████████████████████▌                                                                                 | 117/256 [02:28<02:55,  1.27s/it]117 tensor(2.2969, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 46%|█████████████████████████████████████████████████████████████████████▏                                                                                | 118/256 [02:29<02:54,  1.27s/it]118 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 46%|█████████████████████████████████████████████████████████████████████▋                                                                                | 119/256 [02:30<02:53,  1.27s/it]119 tensor(2.2344, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 47%|██████████████████████████████████████████████████████████████████████▎                                                                               | 120/256 [02:32<02:52,  1.27s/it]120 tensor(1.9766, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 47%|██████████████████████████████████████████████████████████████████████▉                                                                               | 121/256 [02:33<02:50,  1.27s/it]121 tensor(1.3828, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 48%|███████████████████████████████████████████████████████████████████████▍                                                                              | 122/256 [02:34<02:49,  1.27s/it]122 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 48%|████████████████████████████████████████████████████████████████████████                                                                              | 123/256 [02:35<02:48,  1.27s/it]123 tensor(1.6562, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 48%|████████████████████████████████████████████████████████████████████████▋                                                                             | 124/256 [02:37<02:47,  1.27s/it]124 tensor(1.7266, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 49%|█████████████████████████████████████████████████████████████████████████▏                                                                            | 125/256 [02:38<02:45,  1.27s/it]125 tensor(2.2031, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 49%|█████████████████████████████████████████████████████████████████████████▊                                                                            | 126/256 [02:39<02:44,  1.26s/it]126 tensor(1.6406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 50%|██████████████████████████████████████████████████████████████████████████▍                                                                           | 127/256 [02:40<02:43,  1.26s/it]127 tensor(1.8828, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 50%|███████████████████████████████████████████████████████████████████████████                                                                           | 128/256 [02:42<02:42,  1.27s/it]128 tensor(1.7891, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 50%|███████████████████████████████████████████████████████████████████████████▌                                                                          | 129/256 [02:43<02:40,  1.27s/it]129 tensor(1.7031, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 51%|████████████████████████████████████████████████████████████████████████████▏                                                                         | 130/256 [02:44<02:39,  1.27s/it]130 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 51%|████████████████████████████████████████████████████████████████████████████▊                                                                         | 131/256 [02:45<02:38,  1.27s/it]131 tensor(1.7188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 52%|█████████████████████████████████████████████████████████████████████████████▎                                                                        | 132/256 [02:47<02:36,  1.27s/it]132 tensor(1.8125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 52%|█████████████████████████████████████████████████████████████████████████████▉                                                                        | 133/256 [02:48<02:35,  1.27s/it]133 tensor(1.9531, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 52%|██████████████████████████████████████████████████████████████████████████████▌                                                                       | 134/256 [02:49<02:34,  1.27s/it]134 tensor(1.3672, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 53%|███████████████████████████████████████████████████████████████████████████████                                                                       | 135/256 [02:51<02:33,  1.27s/it]135 tensor(0.6289, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 53%|███████████████████████████████████████████████████████████████████████████████▋                                                                      | 136/256 [02:52<02:31,  1.27s/it]136 tensor(1.6406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 54%|████████████████████████████████████████████████████████████████████████████████▎                                                                     | 137/256 [02:53<02:30,  1.27s/it]137 tensor(2.0938, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 54%|████████████████████████████████████████████████████████████████████████████████▊                                                                     | 138/256 [02:54<02:29,  1.27s/it]138 tensor(1.4297, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 54%|█████████████████████████████████████████████████████████████████████████████████▍                                                                    | 139/256 [02:56<02:28,  1.27s/it]139 tensor(2.1719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 55%|██████████████████████████████████████████████████████████████████████████████████                                                                    | 140/256 [02:57<02:26,  1.27s/it]140 tensor(1.9922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 55%|██████████████████████████████████████████████████████████████████████████████████▌                                                                   | 141/256 [02:58<02:25,  1.27s/it]141 tensor(1.5625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 55%|███████████████████████████████████████████████████████████████████████████████████▏                                                                  | 142/256 [02:59<02:24,  1.27s/it]142 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 56%|███████████████████████████████████████████████████████████████████████████████████▊                                                                  | 143/256 [03:01<02:23,  1.27s/it]143 tensor(1.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 56%|████████████████████████████████████████████████████████████████████████████████████▍                                                                 | 144/256 [03:02<02:21,  1.27s/it]144 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 57%|████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 145/256 [03:03<02:20,  1.27s/it]145 tensor(1.7891, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 57%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                | 146/256 [03:04<02:19,  1.27s/it]146 tensor(1.8203, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 57%|██████████████████████████████████████████████████████████████████████████████████████▏                                                               | 147/256 [03:06<02:18,  1.27s/it]147 tensor(1.0547, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 58%|██████████████████████████████████████████████████████████████████████████████████████▋                                                               | 148/256 [03:07<02:16,  1.27s/it]148 tensor(1.3438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 58%|███████████████████████████████████████████████████████████████████████████████████████▎                                                              | 149/256 [03:08<02:15,  1.27s/it]149 tensor(1.6328, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 59%|███████████████████████████████████████████████████████████████████████████████████████▉                                                              | 150/256 [03:10<02:14,  1.27s/it]150 tensor(1.3672, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 59%|████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 151/256 [03:11<02:12,  1.27s/it]151 tensor(1.6875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 59%|█████████████████████████████████████████████████████████████████████████████████████████                                                             | 152/256 [03:12<02:11,  1.27s/it]152 tensor(1.5547, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 60%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 153/256 [03:13<02:10,  1.27s/it]153 tensor(1.7812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 60%|██████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 154/256 [03:15<02:09,  1.27s/it]154 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 61%|██████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 155/256 [03:16<02:07,  1.27s/it]155 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 61%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                          | 156/256 [03:17<02:06,  1.27s/it]156 tensor(1.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 61%|███████████████████████████████████████████████████████████████████████████████████████████▉                                                          | 157/256 [03:18<02:05,  1.27s/it]157 tensor(1.6172, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 62%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 158/256 [03:20<02:04,  1.27s/it]158 tensor(1.9141, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 62%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                        | 159/256 [03:21<02:02,  1.27s/it]159 tensor(2.2031, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 62%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 160/256 [03:22<02:01,  1.27s/it]160 tensor(2.1250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 63%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 161/256 [03:23<02:00,  1.27s/it]161 tensor(1.9609, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 63%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 162/256 [03:25<01:59,  1.27s/it]162 tensor(1.8359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 64%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 163/256 [03:26<01:57,  1.27s/it]163 tensor(2.2188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 64%|████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 164/256 [03:27<01:56,  1.27s/it]164 tensor(1.8359, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 64%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                     | 165/256 [03:29<01:55,  1.27s/it]165 tensor(2.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 166/256 [03:30<01:53,  1.27s/it]166 tensor(1.9922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████▊                                                    | 167/256 [03:31<01:52,  1.27s/it]167 tensor(2.2031, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 66%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 168/256 [03:32<01:51,  1.27s/it]168 tensor(2.0156, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 169/256 [03:34<01:50,  1.27s/it]169 tensor(1.9062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 170/256 [03:35<01:48,  1.27s/it]170 tensor(1.4688, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                 | 171/256 [03:36<01:47,  1.27s/it]171 tensor(1.5156, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 172/256 [03:37<01:46,  1.27s/it]172 tensor(0.8516, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 173/256 [03:39<01:45,  1.27s/it]173 tensor(1.1094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                | 174/256 [03:40<01:43,  1.27s/it]174 tensor(1.6250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 175/256 [03:41<01:42,  1.27s/it]175 tensor(2.2188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                              | 176/256 [03:42<01:41,  1.27s/it]176 tensor(0.6094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                              | 177/256 [03:44<01:40,  1.27s/it]177 tensor(1.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                             | 178/256 [03:45<01:38,  1.27s/it]178 tensor(1.9219, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                             | 179/256 [03:46<01:37,  1.27s/it]179 tensor(2.1094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 180/256 [03:47<01:36,  1.27s/it]180 tensor(1.3203, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 181/256 [03:49<01:34,  1.27s/it]181 tensor(1.9531, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                           | 182/256 [03:50<01:33,  1.27s/it]182 tensor(1.6641, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                          | 183/256 [03:51<01:32,  1.27s/it]183 tensor(1.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 184/256 [03:53<01:31,  1.27s/it]184 tensor(2.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 185/256 [03:54<01:29,  1.27s/it]185 tensor(1.5234, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                         | 186/256 [03:55<01:28,  1.27s/it]186 tensor(2., device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)
 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 187/256 [03:56<01:27,  1.27s/it]187 tensor(2.0469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                       | 188/256 [03:58<01:26,  1.27s/it]188 tensor(2.5781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 189/256 [03:59<01:24,  1.27s/it]189 tensor(0.8633, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 190/256 [04:00<01:23,  1.27s/it]190 tensor(1.2656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                      | 191/256 [04:01<01:22,  1.27s/it]191 tensor(1.9141, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 192/256 [04:03<01:21,  1.27s/it]192 tensor(2.0938, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 193/256 [04:04<01:19,  1.27s/it]193 tensor(2.1094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                    | 194/256 [04:05<01:18,  1.27s/it]194 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 195/256 [04:06<01:17,  1.27s/it]195 tensor(1.8281, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 196/256 [04:08<01:15,  1.27s/it]196 tensor(1.5234, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 197/256 [04:09<01:14,  1.27s/it]197 tensor(1.3438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 198/256 [04:10<01:13,  1.27s/it]198 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 199/256 [04:12<01:12,  1.27s/it]199 tensor(1.2656, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 200/256 [04:13<01:10,  1.27s/it]200 tensor(1.9219, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 201/256 [04:14<01:09,  1.27s/it]201 tensor(2.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 202/256 [04:15<01:08,  1.27s/it]202 tensor(2.0469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 203/256 [04:17<01:07,  1.27s/it]203 tensor(2.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 204/256 [04:18<01:05,  1.27s/it]204 tensor(1.9844, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 205/256 [04:19<01:04,  1.27s/it]205 tensor(1.7812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                             | 206/256 [04:20<01:03,  1.27s/it]206 tensor(0.8320, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 207/256 [04:22<01:02,  1.27s/it]207 tensor(2.0469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 208/256 [04:23<01:00,  1.27s/it]208 tensor(1.7344, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 209/256 [04:24<00:59,  1.27s/it]209 tensor(1.6562, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 210/256 [04:25<00:58,  1.27s/it]210 tensor(2.2188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 211/256 [04:27<00:56,  1.27s/it]211 tensor(1.3438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                         | 212/256 [04:28<00:55,  1.27s/it]212 tensor(1.8516, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 213/256 [04:29<00:54,  1.27s/it]213 tensor(1.8125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 214/256 [04:31<00:53,  1.27s/it]214 tensor(2., device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)
 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                        | 215/256 [04:32<00:51,  1.27s/it]215 tensor(2.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 216/256 [04:33<00:50,  1.27s/it]216 tensor(1.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                      | 217/256 [04:34<00:49,  1.27s/it]217 tensor(0.6328, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 218/256 [04:36<00:48,  1.27s/it]218 tensor(2.0469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                     | 219/256 [04:37<00:46,  1.27s/it]219 tensor(1.5625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 220/256 [04:38<00:45,  1.27s/it]220 tensor(2.0156, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 221/256 [04:39<00:44,  1.27s/it]221 tensor(0.9453, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 222/256 [04:41<00:43,  1.27s/it]222 tensor(1.5391, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 223/256 [04:42<00:41,  1.27s/it]223 tensor(0.3691, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 224/256 [04:43<00:40,  1.27s/it]224 tensor(2.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                  | 225/256 [04:44<00:39,  1.27s/it]225 tensor(1.8594, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 226/256 [04:46<00:37,  1.27s/it]226 tensor(1.8438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 227/256 [04:47<00:36,  1.27s/it]227 tensor(2.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 228/256 [04:48<00:35,  1.27s/it]228 tensor(0.8320, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 229/256 [04:50<00:34,  1.27s/it]229 tensor(1.8750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 230/256 [04:51<00:32,  1.27s/it]230 tensor(2.1719, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 231/256 [04:52<00:31,  1.27s/it]231 tensor(2.1875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 232/256 [04:53<00:30,  1.27s/it]232 tensor(1.9922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 233/256 [04:55<00:29,  1.27s/it]233 tensor(1.4922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 234/256 [04:56<00:27,  1.27s/it]234 tensor(1.5625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 235/256 [04:57<00:26,  1.27s/it]235 tensor(0.8594, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 236/256 [04:58<00:25,  1.27s/it]236 tensor(1.5469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 237/256 [05:00<00:24,  1.27s/it]237 tensor(1.8438, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 238/256 [05:01<00:22,  1.27s/it]238 tensor(1.8828, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 239/256 [05:02<00:21,  1.27s/it]239 tensor(1.7109, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 240/256 [05:03<00:20,  1.27s/it]240 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 241/256 [05:05<00:18,  1.27s/it]241 tensor(2.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 242/256 [05:06<00:17,  1.27s/it]242 tensor(1.7188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 243/256 [05:07<00:16,  1.27s/it]243 tensor(2.4531, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 244/256 [05:09<00:15,  1.27s/it]244 tensor(1.8281, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 245/256 [05:10<00:13,  1.27s/it]245 tensor(1.1484, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 246/256 [05:11<00:12,  1.27s/it]246 tensor(0.1226, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 247/256 [05:12<00:11,  1.27s/it]247 tensor(1.4922, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 248/256 [05:14<00:10,  1.27s/it]248 tensor(1.6875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 249/256 [05:15<00:08,  1.27s/it]249 tensor(1.2188, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 250/256 [05:16<00:07,  1.27s/it]250 tensor(1.8984, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 251/256 [05:17<00:06,  1.27s/it]251 tensor(0.6641, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 252/256 [05:19<00:05,  1.27s/it]252 tensor(2.0156, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 253/256 [05:20<00:03,  1.27s/it]253 tensor(1.1094, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 254/256 [05:21<00:02,  1.27s/it]254 tensor(1.9688, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 255/256 [05:22<00:01,  1.27s/it]255 tensor(2.0781, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [05:24<00:00,  1.27s/it]
sanity check for i_norm and o_norm
i_norm:  torch.Size([4096]) tensor([0.3051, 0.0677, 0.0027,  ..., 0.0376, 0.0432, 0.0179], device='cuda:0')
o_norm:  torch.Size([4096]) tensor([4.9405e-06, 3.5153e-06, 6.9034e-06,  ..., 7.4447e-04, 1.5995e-04,
        8.8798e-05], device='cuda:0')
Starting layer-wise PTQ...
collecting activations...
Ready.
collecting FP target output activations for block 0...
0 self_attn.q_proj
Factorizing 0.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 0.003014392452314496 421.8809509277344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21634480341566795 661.1497192382812 

0 self_attn.v_proj
Factorizing 0.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 0.08099812269210815 57.862525939941406
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21996402740478516 450.486328125 

0 self_attn.o_proj
Factorizing 0.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 0.28940558433532715 697.64892578125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19208417115388093 165.96072387695312 

0 self_attn.k_proj
err before 7.96518143602043e-05
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d668f110>
epoch: 0, train err: 5.696774171326524e-05
epoch: 1, train err: 4.8225242963439996e-05
epoch: 2, train err: 4.841455518089788e-05
epoch: 3, train err: 3.507431505056502e-05
Factorizing 0.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 0.0013751982478424907 44.3394775390625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.11445932665306503 441.35516357421875 

0 mlp.up_proj
Factorizing 0.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 1.1863993406295776 91.11834716796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16566340128580728 1940.249755859375 

0 mlp.gate_proj
Factorizing 0.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 1.0245869159698486 103.46696472167969
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15671925795705696 1905.7061767578125 

0 mlp.down_proj
Factorizing 0.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 6.535452365875244 177.99005126953125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19287357330322266 2468.78173828125 

collecting Q input activations for next block ...
collecting FP target output activations for block 1...
1 self_attn.q_proj
err before 0.24196574330562726
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4e06450>
epoch: 0, train err: 0.10194196126940369
epoch: 1, train err: 0.1707332292035062
epoch: 2, train err: 0.03747813856534776
epoch: 3, train err: 0.015336660737375496
Factorizing 1.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 0.16311073303222656 520.151123046875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.10256852274355681 1207.846923828125 

1 self_attn.v_proj
Factorizing 1.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.632723331451416 147.87838745117188
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.2644853232041845 448.5671081542969 

1 self_attn.o_proj
Factorizing 1.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 5.2718682289123535 504.5146789550781
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1535259860378879 175.63372802734375 

1 self_attn.k_proj
err before 0.07123035110271303
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d459cf90>
epoch: 0, train err: 0.0492208088926418
epoch: 1, train err: 0.03255802314924949
epoch: 2, train err: 0.027508004287483345
epoch: 3, train err: 0.026111168539500795
Factorizing 1.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 0.14860716462135315 819.8374633789062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.10024726325696935 1174.095947265625 

1 mlp.up_proj
Factorizing 1.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 11.50408935546875 100.22467041015625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16145317475377666 2180.263671875 

1 mlp.gate_proj
Factorizing 1.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 7.494961261749268 80.24820709228516
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.13655233981719078 2088.70458984375 

1 mlp.down_proj
Factorizing 1.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 51.76814270019531 3591864.0
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.24362454349047516 3414.6416015625 

collecting Q input activations for next block ...
collecting FP target output activations for block 2...
2 self_attn.q_proj
err before 0.06547315725765657
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f39c8933190>
epoch: 0, train err: 0.057094421310466714
epoch: 1, train err: 0.05291124404539005
epoch: 2, train err: 0.05192987783084391
epoch: 3, train err: 0.051647538883116795
Factorizing 2.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.1371021270751953 145.63731384277344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.12971500661921373 1552.42919921875 

2 self_attn.v_proj
Factorizing 2.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 910.5941162109375 7923.41259765625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17866588521886756 617.4692993164062 

2 self_attn.o_proj
Factorizing 2.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 18.03952980041504 161.13986206054688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21163884054851062 687.4029541015625 

2 self_attn.k_proj
err before 0.057836925341689494
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d459cf10>
epoch: 0, train err: 0.055301112373854266
epoch: 1, train err: 0.053692290082835825
epoch: 2, train err: 0.05314467053540284
epoch: 3, train err: 0.05296129288035445
Factorizing 2.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 1.1740667819976807 306.2489013671875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.11707408611591046 1558.490234375 

2 mlp.up_proj
Factorizing 2.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 27.454116821289062 178.94288635253906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17159454975653132 2394.087158203125 

2 mlp.gate_proj
Factorizing 2.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 20.53823471069336 151.5331268310547
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15552635798378597 2508.3291015625 

2 mlp.down_proj
Factorizing 2.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 47.56281280517578 310.7423095703125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18993564105141755 2686.44970703125 

collecting Q input activations for next block ...
collecting FP target output activations for block 3...
3 self_attn.q_proj
err before 0.0647763983142795
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d66b8c90>
epoch: 0, train err: 0.05707147019711556
epoch: 1, train err: 0.053023826629214454
epoch: 2, train err: 0.051964274294732604
epoch: 3, train err: 0.051645504507177975
Factorizing 3.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.3768911361694336 197.7361602783203
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16125593356743545 1723.50341796875 

3 self_attn.v_proj
Factorizing 3.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1420.42578125 10533.4521484375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18108140994340946 564.9739990234375 

3 self_attn.o_proj
Factorizing 3.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 19.56087303161621 192.02395629882812
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19597410386608494 583.2189331054688 

3 self_attn.k_proj
err before 0.06116319868306164
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4719b90>
epoch: 0, train err: 0.05627950676716864
epoch: 1, train err: 0.05405117886402877
epoch: 2, train err: 0.05344709642668022
epoch: 3, train err: 0.053257706262229476
Factorizing 3.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.8134474754333496 363.2417297363281
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16127858240960052 1868.2510986328125 

3 mlp.up_proj
Factorizing 3.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 38.02296829223633 238.24876403808594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18452674830885238 2586.326904296875 

3 mlp.gate_proj
Factorizing 3.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 31.664644241333008 209.68978881835938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17487619817256927 2865.171630859375 

3 mlp.down_proj
Factorizing 3.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 50.96904754638672 333.6090393066406
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18161949610601277 2545.578857421875 

collecting Q input activations for next block ...
collecting FP target output activations for block 4...
4 self_attn.q_proj
err before 0.044577827968169004
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d44f4310>
epoch: 0, train err: 0.03908993372169789
epoch: 1, train err: 0.03576275992963929
epoch: 2, train err: 0.03470673450647155
epoch: 3, train err: 0.034342004073550925
Factorizing 4.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 2.677523136138916 172.5204315185547
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.68it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1554659637956988 1800.917724609375 

4 self_attn.v_proj
Factorizing 4.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 823.2943725585938 5843.68017578125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18126527285246255 629.35302734375 

4 self_attn.o_proj
Factorizing 4.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 24.717863082885742 196.32444763183594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19405781474090428 642.719482421875 

4 self_attn.k_proj
err before 0.04311081868945621
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4723d50>
epoch: 0, train err: 0.03915387271263171
epoch: 1, train err: 0.03714652656344697
epoch: 2, train err: 0.03653849204420112
epoch: 3, train err: 0.036363333943882026
Factorizing 4.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.0414106845855713 349.72174072265625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1547264299894634 1881.473388671875 

4 mlp.up_proj
Factorizing 4.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 40.80447769165039 273.2830810546875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18803505121275435 2587.3623046875 

4 mlp.gate_proj
Factorizing 4.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 32.56122970581055 254.60140991210938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17580904279436385 2992.97314453125 

4 mlp.down_proj
Factorizing 4.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 54.112892150878906 380.0201416015625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18256804416996772 2488.767578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 5...
5 self_attn.q_proj
err before 0.0422327802007203
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d459e3d0>
epoch: 0, train err: 0.03775124230014626
epoch: 1, train err: 0.034958016221935395
epoch: 2, train err: 0.03406508129410213
epoch: 3, train err: 0.033778634868212976
Factorizing 5.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.284482479095459 227.43368530273438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17220248029531676 2016.83544921875 

5 self_attn.v_proj
Factorizing 5.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 529.67626953125 3649.776611328125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18095433084588303 660.1213989257812 

5 self_attn.o_proj
Factorizing 5.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 21.72738265991211 195.96156311035156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19880732265087442 693.43994140625 

5 self_attn.k_proj
err before 0.040517869041650556
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d477fb50>
epoch: 0, train err: 0.03785929616424255
epoch: 1, train err: 0.03623393636371475
epoch: 2, train err: 0.035692186531377956
epoch: 3, train err: 0.035515610812581144
Factorizing 5.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.407088279724121 340.5155029296875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1683289034282742 2143.8369140625 

5 mlp.up_proj
Factorizing 5.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 39.720001220703125 252.55551147460938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18735511280665887 2566.015625 

5 mlp.gate_proj
Factorizing 5.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 30.02775001525879 226.69525146484375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17355810706295186 2976.86865234375 

5 mlp.down_proj
Factorizing 5.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 46.974220275878906 292.7471008300781
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17869181028554137 2435.9267578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 6...
6 self_attn.q_proj
err before 0.03975435782922432
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4593810>
epoch: 0, train err: 0.035627407603897154
epoch: 1, train err: 0.03324139668256976
epoch: 2, train err: 0.03245173639879795
epoch: 3, train err: 0.032166223325475585
Factorizing 6.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 3.4210896492004395 189.47064208984375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.68it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1550358029230972 1617.33349609375 

6 self_attn.v_proj
Factorizing 6.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 336.90008544921875 2578.744140625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17641908295300543 553.250244140625 

6 self_attn.o_proj
Factorizing 6.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 18.585533142089844 156.92112731933594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18095913937217312 550.1157836914062 

6 self_attn.k_proj
err before 0.03721751642297022
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d44eb190>
epoch: 0, train err: 0.0351055975042982
epoch: 1, train err: 0.033739269245415926
epoch: 2, train err: 0.03326883218687726
epoch: 3, train err: 0.033112889585027006
Factorizing 6.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 4.403050899505615 334.1004333496094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15682428023394415 1706.2481689453125 

6 mlp.up_proj
Factorizing 6.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 38.99897766113281 249.02059936523438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18713979901007885 2539.11279296875 

6 mlp.gate_proj
Factorizing 6.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 28.10005760192871 205.88510131835938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17168418220851733 3032.62939453125 

6 mlp.down_proj
Factorizing 6.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 43.552833557128906 275.5601806640625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17783323015485492 2390.07861328125 

collecting Q input activations for next block ...
collecting FP target output activations for block 7...
7 self_attn.q_proj
err before 0.03871055966737913
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d459c310>
epoch: 0, train err: 0.03499172633019043
epoch: 1, train err: 0.03289943925483385
epoch: 2, train err: 0.032199385335843544
epoch: 3, train err: 0.0319618304274627
Factorizing 7.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.1838889122009277 166.52476501464844
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15390180658411096 1595.6539306640625 

7 self_attn.v_proj
Factorizing 7.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 247.05722045898438 1993.263427734375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1770360576572703 569.3479614257812 

7 self_attn.o_proj
Factorizing 7.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 17.399150848388672 150.03497314453125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19245050855251175 594.2871704101562 

7 self_attn.k_proj
err before 0.03621751423634123
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47199d0>
epoch: 0, train err: 0.034588872724270914
epoch: 1, train err: 0.03340808329085121
epoch: 2, train err: 0.03298915440245764
epoch: 3, train err: 0.03284911529044621
Factorizing 7.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.9371109008789062 288.494384765625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15481601110318813 1624.9488525390625 

7 mlp.up_proj
Factorizing 7.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 35.783321380615234 233.21556091308594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18582373605647556 2533.149169921875 

7 mlp.gate_proj
Factorizing 7.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 25.302385330200195 188.111572265625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16882556417713995 2982.134765625 

7 mlp.down_proj
Factorizing 7.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 39.4984130859375 252.7573699951172
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1751529096992095 2365.264892578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 8...
8 self_attn.q_proj
err before 0.039962103284779005
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f36a42e9b90>
epoch: 0, train err: 0.03645378469082061
epoch: 1, train err: 0.03438723940780619
epoch: 2, train err: 0.03369173933606362
epoch: 3, train err: 0.0334662130480865
Factorizing 8.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 2.8881869316101074 152.12030029296875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1599189295913234 1688.743896484375 

8 self_attn.v_proj
Factorizing 8.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 171.62423706054688 1395.80517578125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17340152668503095 588.177978515625 

8 self_attn.o_proj
Factorizing 8.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 18.552265167236328 157.47540283203125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19511013404995786 636.8394775390625 

8 self_attn.k_proj
err before 0.03762258921051398
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4722250>
epoch: 0, train err: 0.03617395510082133
epoch: 1, train err: 0.03505251501337625
epoch: 2, train err: 0.03465139219042612
epoch: 3, train err: 0.03451718278665794
Factorizing 8.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.4147448539733887 302.94305419921875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16538034166608537 1778.16943359375 

8 mlp.up_proj
Factorizing 8.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 31.320960998535156 206.50967407226562
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18420403156805476 2570.0146484375 

8 mlp.gate_proj
Factorizing 8.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 22.381542205810547 164.08551025390625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16710177464271658 2866.129638671875 

8 mlp.down_proj
Factorizing 8.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 35.77250671386719 229.82679748535156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1736141648403434 2388.930908203125 

collecting Q input activations for next block ...
collecting FP target output activations for block 9...
9 self_attn.q_proj
err before 0.04276682132331189
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4721d90>
epoch: 0, train err: 0.03937950017279945
epoch: 1, train err: 0.03741784366866341
epoch: 2, train err: 0.036749839899130166
epoch: 3, train err: 0.03652876448177267
Factorizing 9.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 3.3718972206115723 129.85659790039062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16383306329900568 1730.0771484375 

9 self_attn.v_proj
Factorizing 9.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 146.76148986816406 1209.605712890625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1744005723433061 613.8900146484375 

9 self_attn.o_proj
Factorizing 9.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 19.114070892333984 153.27566528320312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19625152160074108 671.9652099609375 

9 self_attn.k_proj
err before 0.04075815148826223
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47b2b90>
epoch: 0, train err: 0.039376363733026665
epoch: 1, train err: 0.038270847813691944
epoch: 2, train err: 0.03787192594609223
epoch: 3, train err: 0.037738758241175674
Factorizing 9.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.8069686889648438 285.9540100097656
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16189792238432785 1802.895263671875 

9 mlp.up_proj
Factorizing 9.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 27.599655151367188 199.85813903808594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1834072613608244 2594.1123046875 

9 mlp.gate_proj
Factorizing 9.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 19.91587257385254 148.16864013671875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16800692609248272 2817.14013671875 

9 mlp.down_proj
Factorizing 9.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 33.20634460449219 212.12973022460938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17357166535263763 2421.671875 

collecting Q input activations for next block ...
collecting FP target output activations for block 10...
10 self_attn.q_proj
err before 0.04605824430473149
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d66c59d0>
epoch: 0, train err: 0.04258699521597009
epoch: 1, train err: 0.040475880770827644
epoch: 2, train err: 0.0397752890639822
epoch: 3, train err: 0.039548437169287354
Factorizing 10.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.68it/s]err 259 3.127028226852417 116.29960632324219
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.68it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15204791444720644 1605.6259765625 

10 self_attn.v_proj
Factorizing 10.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 89.73687744140625 718.8372802734375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16990079657976018 584.458740234375 

10 self_attn.o_proj
Factorizing 10.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 21.20874786376953 169.5518798828125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17747378799150576 601.9910888671875 

10 self_attn.k_proj
err before 0.04391229580505751
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4719c50>
epoch: 0, train err: 0.04253892242559232
epoch: 1, train err: 0.041450002245255746
epoch: 2, train err: 0.04104443397955038
epoch: 3, train err: 0.0409084811253706
Factorizing 10.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.393885612487793 272.0459899902344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15056607939980246 1695.976318359375 

10 mlp.up_proj
Factorizing 10.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 25.78223419189453 174.71461486816406
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1830484845996958 2647.61328125 

10 mlp.gate_proj
Factorizing 10.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 18.449861526489258 139.51785278320312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16858523442195011 2805.25830078125 

10 mlp.down_proj
Factorizing 10.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 32.567413330078125 207.4638214111328
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17193031311035156 2442.785888671875 

collecting Q input activations for next block ...
collecting FP target output activations for block 11...
11 self_attn.q_proj
err before 0.04630981008813251
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4517090>
epoch: 0, train err: 0.043209205323364586
epoch: 1, train err: 0.04123037459066836
epoch: 2, train err: 0.04055062342376914
epoch: 3, train err: 0.040331050535314716
Factorizing 11.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 2.968548059463501 123.45643615722656
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.14294105888213088 1363.0859375 

11 self_attn.v_proj
Factorizing 11.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 98.14723205566406 838.7720947265625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16697174463516626 625.1422119140625 

11 self_attn.o_proj
Factorizing 11.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 18.612770080566406 153.1796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1727681055860228 633.0223388671875 

11 self_attn.k_proj
err before 0.043920022624661215
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d470d150>
epoch: 0, train err: 0.04284298216953175
epoch: 1, train err: 0.04184233387786662
epoch: 2, train err: 0.041459991902229376
epoch: 3, train err: 0.04133088006346952
Factorizing 11.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.292947292327881 280.69097900390625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15032458949733424 1423.87451171875 

11 mlp.up_proj
Factorizing 11.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 24.609817504882812 164.448486328125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18160058726435122 2673.16064453125 

11 mlp.gate_proj
Factorizing 11.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 17.40970230102539 127.31076049804688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16696360684180445 2756.903076171875 

11 mlp.down_proj
Factorizing 11.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 31.619413375854492 199.00387573242188
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17382181803385416 2503.0341796875 

collecting Q input activations for next block ...
collecting FP target output activations for block 12...
12 self_attn.q_proj
err before 0.04769341569044627
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d66b1150>
epoch: 0, train err: 0.04500809314777143
epoch: 1, train err: 0.04315801348275272
epoch: 2, train err: 0.04249912899103947
epoch: 3, train err: 0.042287081058020703
Factorizing 12.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 3.1374964714050293 105.97865295410156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1637864512781943 1624.7615966796875 

12 self_attn.v_proj
Factorizing 12.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 86.79995727539062 720.6917724609375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17492910017047011 638.141357421875 

12 self_attn.o_proj
Factorizing 12.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 19.117284774780273 143.412353515625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18595777239118302 666.47265625 

12 self_attn.k_proj
err before 0.04583474065293558
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d477f6d0>
epoch: 0, train err: 0.044873392034787685
epoch: 1, train err: 0.04391256674716715
epoch: 2, train err: 0.043535567325307056
epoch: 3, train err: 0.043408077355707064
Factorizing 12.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 3.1787872314453125 233.21209716796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15776605317086884 1666.009521484375 

12 mlp.up_proj
Factorizing 12.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 24.43299102783203 161.98081970214844
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18115113739274505 2712.91943359375 

12 mlp.gate_proj
Factorizing 12.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 17.583595275878906 126.15853881835938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1673437978707108 2731.05078125 

12 mlp.down_proj
Factorizing 12.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 31.62236785888672 198.34420776367188
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17495697423031456 2552.97216796875 

collecting Q input activations for next block ...
collecting FP target output activations for block 13...
13 self_attn.q_proj
err before 0.05113075174449477
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47d5f10>
epoch: 0, train err: 0.048179410528973676
epoch: 1, train err: 0.04620332196645904
epoch: 2, train err: 0.04549206925003091
epoch: 3, train err: 0.045264712898642756
Factorizing 13.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 3.0151820182800293 96.31451416015625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16586461820100484 1613.531005859375 

13 self_attn.v_proj
Factorizing 13.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 89.66537475585938 686.3544921875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18421001823580996 722.103271484375 

13 self_attn.o_proj
Factorizing 13.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 18.444272994995117 141.6195068359375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18258546193440756 701.128173828125 

13 self_attn.k_proj
err before 0.04914071781968232
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d471fa10>
epoch: 0, train err: 0.048016030254075304
epoch: 1, train err: 0.046952555305324495
epoch: 2, train err: 0.046542832409613766
epoch: 3, train err: 0.046404628606978804
Factorizing 13.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 3.489767551422119 213.72149658203125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16629728486266318 1681.59814453125 

13 mlp.up_proj
Factorizing 13.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 25.663928985595703 181.66993713378906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18332870036488297 2804.19580078125 

13 mlp.gate_proj
Factorizing 13.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 18.439647674560547 132.47572326660156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16996689747444726 2752.10400390625 

13 mlp.down_proj
Factorizing 13.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 35.238338470458984 221.15374755859375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17892382062714676 2656.660888671875 

collecting Q input activations for next block ...
collecting FP target output activations for block 14...
14 self_attn.q_proj
err before 0.05408843347686343
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d46e7d90>
epoch: 0, train err: 0.05133447652042378
epoch: 1, train err: 0.049291325150988996
epoch: 2, train err: 0.048547586280619726
epoch: 3, train err: 0.04830531356856227
Factorizing 14.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.901580333709717 89.21005249023438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15563599794905708 1504.0662841796875 

14 self_attn.v_proj
Factorizing 14.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 71.97660827636719 520.37744140625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17851406032756223 674.069091796875 

14 self_attn.o_proj
Factorizing 14.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 20.035402297973633 147.80230712890625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17939117563751353 663.02978515625 

14 self_attn.k_proj
err before 0.05200302186131012
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1a942490>
epoch: 0, train err: 0.05109115058439784
epoch: 1, train err: 0.050044288524077274
epoch: 2, train err: 0.04961821357574081
epoch: 3, train err: 0.049473978659079876
Factorizing 14.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.942162036895752 200.32904052734375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.15345435806467564 1551.73046875 

14 mlp.up_proj
Factorizing 14.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 26.08637237548828 172.0955047607422
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18126993179321288 2784.30615234375 

14 mlp.gate_proj
Factorizing 14.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 19.363441467285156 135.74713134765625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16881028432694692 2722.572265625 

14 mlp.down_proj
Factorizing 14.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 36.90401077270508 231.43589782714844
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18013842013772466 2686.22412109375 

collecting Q input activations for next block ...
collecting FP target output activations for block 15...
15 self_attn.q_proj
err before 0.05975683656288311
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f39c8aa42d0>
epoch: 0, train err: 0.05664607560902368
epoch: 1, train err: 0.05440224207995925
epoch: 2, train err: 0.05358854263613466
epoch: 3, train err: 0.05332257258123718
Factorizing 15.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 3.3258447647094727 105.54325866699219
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16484524752642657 1561.4141845703125 

15 self_attn.v_proj
Factorizing 15.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 80.8751220703125 569.8684692382812
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18021953105926514 738.17919921875 

15 self_attn.o_proj
Factorizing 15.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 20.530254364013672 144.02102661132812
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1823611278610536 726.5267333984375 

15 self_attn.k_proj
err before 0.05732176124001853
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d475df10>
epoch: 0, train err: 0.05627229927631561
epoch: 1, train err: 0.05512076654122211
epoch: 2, train err: 0.05465452489443123
epoch: 3, train err: 0.05449605539615732
Factorizing 15.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 3.2227730751037598 216.15115356445312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16188282302663295 1636.9591064453125 

15 mlp.up_proj
Factorizing 15.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 28.694250106811523 192.12026977539062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18492687635185304 2864.1474609375 

15 mlp.gate_proj
Factorizing 15.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 21.666234970092773 158.5958251953125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17135437610102636 2796.50341796875 

15 mlp.down_proj
Factorizing 15.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 42.53782653808594 264.6874084472656
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18524666644157248 2786.10986328125 

collecting Q input activations for next block ...
collecting FP target output activations for block 16...
16 self_attn.q_proj
err before 0.06745667416544165
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d66c5f10>
epoch: 0, train err: 0.06461655657039955
epoch: 1, train err: 0.06209568491613027
epoch: 2, train err: 0.06115089500963222
epoch: 3, train err: 0.060843819490401074
Factorizing 16.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 2.9409279823303223 85.98707580566406
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16240046775504335 1517.469970703125 

16 self_attn.v_proj
Factorizing 16.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 76.42694854736328 504.2235107421875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1863678993938638 828.9644165039062 

16 self_attn.o_proj
Factorizing 16.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 25.314687728881836 164.9874725341797
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19452984192792108 846.5938720703125 

16 self_attn.k_proj
err before 0.06691549994866364
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47ba490>
epoch: 0, train err: 0.06536740819865372
epoch: 1, train err: 0.06373550534772221
epoch: 2, train err: 0.06312697936664335
epoch: 3, train err: 0.06292571971425787
Factorizing 16.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 2.853379964828491 189.70693969726562
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1591271363295518 1568.3570556640625 

16 mlp.up_proj
Factorizing 16.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 30.53324317932129 213.83946228027344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18233819441361862 2824.053955078125 

16 mlp.gate_proj
Factorizing 16.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 23.518280029296875 159.7576446533203
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16913051753081093 2792.68310546875 

16 mlp.down_proj
Factorizing 16.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 51.40094757080078 320.07574462890625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18846753708859706 2834.5517578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 17...
17 self_attn.q_proj
err before 0.07688458519987762
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d477c3d0>
epoch: 0, train err: 0.0742846368229948
epoch: 1, train err: 0.0717481072206283
epoch: 2, train err: 0.07074991401168518
epoch: 3, train err: 0.07042089932656381
Factorizing 17.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 2.620584487915039 72.46170043945312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18062012679093367 1653.035400390625 

17 self_attn.v_proj
Factorizing 17.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 90.31031799316406 563.7576904296875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18934244349382925 836.13623046875 

17 self_attn.o_proj
Factorizing 17.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 17.306764602661133 109.28823852539062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20038716933306525 872.0849609375 

17 self_attn.k_proj
err before 0.07421482329664286
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d43ea490>
epoch: 0, train err: 0.0733205380383879
epoch: 1, train err: 0.07194441960018594
epoch: 2, train err: 0.07135706272674724
epoch: 3, train err: 0.07115440975758247
Factorizing 17.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.7106871604919434 131.84494018554688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1737253697713216 1667.7635498046875 

17 mlp.up_proj
Factorizing 17.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 32.22288131713867 202.02215576171875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18081607818603515 2777.3349609375 

17 mlp.gate_proj
Factorizing 17.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 25.428390502929688 165.98117065429688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16731168284560693 2826.898193359375 

17 mlp.down_proj
Factorizing 17.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 52.55287551879883 316.95086669921875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18737636468349358 2806.1484375 

collecting Q input activations for next block ...
collecting FP target output activations for block 18...
18 self_attn.q_proj
err before 0.08715760291670449
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1a92c450>
epoch: 0, train err: 0.08451000976492651
epoch: 1, train err: 0.08186867565382272
epoch: 2, train err: 0.08080599337699823
epoch: 3, train err: 0.08045404018776026
Factorizing 18.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.592377185821533 68.7716293334961
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18848137959946681 1652.604736328125 

18 self_attn.v_proj
Factorizing 18.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 113.46204376220703 696.2592163085938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19036756603923066 919.8560791015625 

18 self_attn.o_proj
Factorizing 18.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 17.92766571044922 113.19888305664062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.2049666164683647 964.1629638671875 

18 self_attn.k_proj
err before 0.08461976041144226
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d43ea490>
epoch: 0, train err: 0.08366117226250935
epoch: 1, train err: 0.08218825841322541
epoch: 2, train err: 0.08155195724975783
epoch: 3, train err: 0.08133162412559614
Factorizing 18.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 2.8617424964904785 119.20262145996094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18363867865668404 1692.4140625 

18 mlp.up_proj
Factorizing 18.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 34.395809173583984 213.07928466796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1822143937753334 2787.1513671875 

18 mlp.gate_proj
Factorizing 18.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 28.057186126708984 178.53732299804688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16940928217190415 2905.7080078125 

18 mlp.down_proj
Factorizing 18.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 54.15093231201172 334.92657470703125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18843065571581197 2821.9375 

collecting Q input activations for next block ...
collecting FP target output activations for block 19...
19 self_attn.q_proj
err before 0.09788569077500142
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4452410>
epoch: 0, train err: 0.09540625879890285
epoch: 1, train err: 0.09275654329394456
epoch: 2, train err: 0.09167989785782993
epoch: 3, train err: 0.09131865181552712
Factorizing 19.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 2.221726417541504 60.00190734863281
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18079851634466826 1550.528076171875 

19 self_attn.v_proj
Factorizing 19.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 100.682861328125 626.8192138671875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18544514148266283 913.8736572265625 

19 self_attn.o_proj
Factorizing 19.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 14.685787200927734 90.00997924804688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20190346319943864 975.5975341796875 

19 self_attn.k_proj
err before 0.0946475364617072
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4435810>
epoch: 0, train err: 0.09383867308497429
epoch: 1, train err: 0.09238691419886891
epoch: 2, train err: 0.09173672611359507
epoch: 3, train err: 0.09151023646700196
Factorizing 19.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.2418689727783203 104.17536163330078
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17347711796383206 1543.25244140625 

19 mlp.up_proj
Factorizing 19.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 34.66584014892578 214.58262634277344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18224076147358786 2787.5546875 

19 mlp.gate_proj
Factorizing 19.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 29.630056381225586 186.41058349609375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17036015545880354 2943.823486328125 

19 mlp.down_proj
Factorizing 19.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 52.52561950683594 324.7623291015625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18909740042179188 2844.02490234375 

collecting Q input activations for next block ...
collecting FP target output activations for block 20...
20 self_attn.q_proj
err before 0.1093545701878611
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d44ea250>
epoch: 0, train err: 0.10697715907008387
epoch: 1, train err: 0.10413206028169952
epoch: 2, train err: 0.10298058402258903
epoch: 3, train err: 0.10259269742527977
Factorizing 20.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.761927843093872 48.742942810058594
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17459678649902344 1519.6904296875 

20 self_attn.v_proj
Factorizing 20.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 93.43357849121094 544.9766845703125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18619440186698483 947.3571166992188 

20 self_attn.o_proj
Factorizing 20.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 15.42572021484375 97.74874877929688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21133471446432125 1061.74560546875 

20 self_attn.k_proj
err before 0.10666041341028176
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d44357d0>
epoch: 0, train err: 0.10555927545647137
epoch: 1, train err: 0.10387584293494001
epoch: 2, train err: 0.10316048734239303
epoch: 3, train err: 0.10291296010836959
Factorizing 20.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.8166043758392334 84.15824890136719
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.166617603166729 1503.5572509765625 

20 mlp.up_proj
Factorizing 20.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 34.30291748046875 209.93162536621094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1804427781364409 2760.052734375 

20 mlp.gate_proj
Factorizing 20.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.901559829711914 188.24142456054688
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16912070504070198 2965.70068359375 

20 mlp.down_proj
Factorizing 20.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 55.944740295410156 347.1954650878906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19219662280792885 2890.63720703125 

collecting Q input activations for next block ...
collecting FP target output activations for block 21...
21 self_attn.q_proj
err before 0.11805824597831815
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1addd950>
epoch: 0, train err: 0.11603003900381736
epoch: 1, train err: 0.11340131610631943
epoch: 2, train err: 0.11230857210466638
epoch: 3, train err: 0.11193630937486887
Factorizing 21.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.4940041303634644 38.78758239746094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17170668381911058 1428.599609375 

21 self_attn.v_proj
Factorizing 21.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 105.55860900878906 629.7002563476562
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18386939009265788 994.3656616210938 

21 self_attn.o_proj
Factorizing 21.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 12.893898010253906 75.10780334472656
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.2045207605129335 1073.324951171875 

21 self_attn.k_proj
err before 0.1148884579888545
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d668cc50>
epoch: 0, train err: 0.11421827785670757
epoch: 1, train err: 0.11271776887588203
epoch: 2, train err: 0.11203333610319532
epoch: 3, train err: 0.11179161356994882
Factorizing 21.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.666649580001831 73.91864013671875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1687172194172565 1436.1209716796875 

21 mlp.up_proj
Factorizing 21.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 33.423667907714844 204.27035522460938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18019612095937007 2744.747314453125 

21 mlp.gate_proj
Factorizing 21.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.82966423034668 184.31967163085938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16992639458697775 3001.579833984375 

21 mlp.down_proj
Factorizing 21.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 50.37493133544922 308.0690612792969
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19120856751786902 2875.77685546875 

collecting Q input activations for next block ...
collecting FP target output activations for block 22...
22 self_attn.q_proj
err before 0.12536065193125978
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4497bd0>
epoch: 0, train err: 0.12336210752255283
epoch: 1, train err: 0.12075047791586258
epoch: 2, train err: 0.11964375909883529
epoch: 3, train err: 0.11926749045960605
Factorizing 22.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.9786937236785889 48.11042022705078
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17508923565899884 1512.77099609375 

22 self_attn.v_proj
Factorizing 22.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 95.75631713867188 564.115234375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18977791281307446 1032.391845703125 

22 self_attn.o_proj
Factorizing 22.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 13.701889038085938 81.65426635742188
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20589988014914773 1087.1513671875 

22 self_attn.k_proj
err before 0.12281954914215021
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d44dc450>
epoch: 0, train err: 0.12201446265680715
epoch: 1, train err: 0.12042731215478852
epoch: 2, train err: 0.11973004176979885
epoch: 3, train err: 0.1194856972724665
Factorizing 22.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.214421510696411 87.30421447753906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17478071528373004 1554.8492431640625 

22 mlp.up_proj
Factorizing 22.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 32.76969528198242 197.11077880859375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17926129573533514 2730.508056640625 

22 mlp.gate_proj
Factorizing 22.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.557708740234375 181.44776916503906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1691413606916155 3031.01318359375 

22 mlp.down_proj
Factorizing 22.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 48.812232971191406 295.76885986328125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1900853583153258 2858.8837890625 

collecting Q input activations for next block ...
collecting FP target output activations for block 23...
23 self_attn.q_proj
err before 0.12847140926169232
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f39c8931b50>
epoch: 0, train err: 0.12687725535943173
epoch: 1, train err: 0.12452888657571748
epoch: 2, train err: 0.12352717915200628
epoch: 3, train err: 0.12318440040689893
Factorizing 23.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.981268286705017 42.23904037475586
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.187413868151213 1595.266845703125 

23 self_attn.v_proj
Factorizing 23.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 116.11449432373047 679.0341796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18852008286342825 1122.071533203125 

23 self_attn.o_proj
Factorizing 23.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 11.467772483825684 66.63038635253906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20258746304354824 1179.869384765625 

23 self_attn.k_proj
err before 0.1257544388936367
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4722490>
epoch: 0, train err: 0.12517360245692544
epoch: 1, train err: 0.12376305778161623
epoch: 2, train err: 0.12312295567244291
epoch: 3, train err: 0.12289692601189017
Factorizing 23.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 2.3036105632781982 73.37673950195312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1832460515639361 1594.9736328125 

23 mlp.up_proj
Factorizing 23.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 32.43281936645508 193.16848754882812
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18026522000630696 2768.873779296875 

23 mlp.gate_proj
Factorizing 23.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.95835304260254 182.62460327148438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17074475969587052 3059.74609375 

23 mlp.down_proj
Factorizing 23.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 43.18054962158203 263.05120849609375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1878632938160616 2861.53369140625 

collecting Q input activations for next block ...
collecting FP target output activations for block 24...
24 self_attn.q_proj
err before 0.13046911271521822
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1af434d0>
epoch: 0, train err: 0.12909531968762167
epoch: 1, train err: 0.12681547374813817
epoch: 2, train err: 0.1258462680852972
epoch: 3, train err: 0.12551426541176625
Factorizing 24.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.6549062728881836 42.9958381652832
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.16662126874166822 1343.6339111328125 

24 self_attn.v_proj
Factorizing 24.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 84.38270568847656 496.4349365234375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18591835980858307 1088.7379150390625 

24 self_attn.o_proj
Factorizing 24.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 11.326469421386719 70.07758331298828
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21068004026251325 1193.291748046875 

24 self_attn.k_proj
err before 0.12804868331295438
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4416a10>
epoch: 0, train err: 0.12754982555634342
epoch: 1, train err: 0.12624058875371702
epoch: 2, train err: 0.12563672169926576
epoch: 3, train err: 0.12542192806722596
Factorizing 24.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.7706763744354248 85.189453125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1666582892922794 1359.931640625 

24 mlp.up_proj
Factorizing 24.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 31.545101165771484 186.31668090820312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18065651192152796 2798.008056640625 

24 mlp.gate_proj
Factorizing 24.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 29.54706382751465 178.83998107910156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17252578735351562 3091.662109375 

24 mlp.down_proj
Factorizing 24.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 38.73562240600586 238.16644287109375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1861445426940918 2859.18017578125 

collecting Q input activations for next block ...
collecting FP target output activations for block 25...
25 self_attn.q_proj
err before 0.1286794387269765
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f36a42ea390>
epoch: 0, train err: 0.12735163865727372
epoch: 1, train err: 0.12538649584166706
epoch: 2, train err: 0.12453716987511143
epoch: 3, train err: 0.12424697179812938
Factorizing 25.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 1.838061809539795 39.727333068847656
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18131254613399506 1485.3123779296875 

25 self_attn.v_proj
Factorizing 25.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 105.1111831665039 607.1219482421875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18880789897163508 1190.2449951171875 

25 self_attn.o_proj
Factorizing 25.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 8.298705101013184 49.730125427246094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20255523920059204 1244.4993896484375 

25 self_attn.k_proj
err before 0.12606947304448113
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d4435910>
epoch: 0, train err: 0.1256404824089259
epoch: 1, train err: 0.12446513943723403
epoch: 2, train err: 0.12391673945239745
epoch: 3, train err: 0.12372135647456162
Factorizing 25.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.269648551940918 68.04454040527344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1831326299859572 1511.9429931640625 

25 mlp.up_proj
Factorizing 25.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 31.69548988342285 188.39952087402344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18238908736432186 2848.18798828125 

25 mlp.gate_proj
Factorizing 25.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 30.134601593017578 182.97079467773438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1736198181801654 3133.490478515625 

25 mlp.down_proj
Factorizing 25.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 34.18558120727539 215.98892211914062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1858191687213488 2877.96728515625 

collecting Q input activations for next block ...
collecting FP target output activations for block 26...
26 self_attn.q_proj
err before 0.12704800805659033
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f39c892d150>
epoch: 0, train err: 0.1257628903258592
epoch: 1, train err: 0.1237832942570094
epoch: 2, train err: 0.12293728720396757
epoch: 3, train err: 0.12264840805437416
Factorizing 26.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 1.8369786739349365 43.651756286621094
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17691486112533078 1403.996337890625 

26 self_attn.v_proj
Factorizing 26.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 79.93760681152344 507.38934326171875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.18881000086591748 1226.509765625 

26 self_attn.o_proj
Factorizing 26.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 9.932668685913086 65.88456726074219
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.2218637798556048 1427.02783203125 

26 self_attn.k_proj
err before 0.1252912923519034
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47ba490>
epoch: 0, train err: 0.12477904604747891
epoch: 1, train err: 0.12363706208998337
epoch: 2, train err: 0.12310901493765414
epoch: 3, train err: 0.12292104150401428
Factorizing 26.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.0995538234710693 77.95556640625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17454384061187622 1413.10693359375 

26 mlp.up_proj
Factorizing 26.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:30<00:00,  2.88it/s]err 259 31.546287536621094 188.1488800048828
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18490471353336257 2899.305908203125 

26 mlp.gate_proj
Factorizing 26.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 30.186954498291016 184.7486114501953
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1740168853544853 3162.930908203125 

26 mlp.down_proj
Factorizing 26.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 31.351577758789062 200.11749267578125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1875100469393004 2916.15625 

collecting Q input activations for next block ...
collecting FP target output activations for block 27...
27 self_attn.q_proj
err before 0.12272520750411786
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d666b810>
epoch: 0, train err: 0.12142668850719929
epoch: 1, train err: 0.11973096293513663
epoch: 2, train err: 0.11899863329017535
epoch: 3, train err: 0.11874827271094546
Factorizing 27.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.9096754789352417 31.236827850341797
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1922028242652096 1648.3314208984375 

27 self_attn.v_proj
Factorizing 27.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 82.05836486816406 471.39935302734375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1933118969786401 1261.9400634765625 

27 self_attn.o_proj
Factorizing 27.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 7.804559707641602 50.64839172363281
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21270785778026863 1381.750244140625 

27 self_attn.k_proj
err before 0.12060096635832451
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d42fb050>
epoch: 0, train err: 0.12013783122529276
epoch: 1, train err: 0.11911062613944523
epoch: 2, train err: 0.11862911441130564
epoch: 3, train err: 0.1184583708527498
Factorizing 27.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 2.4371960163116455 52.60240173339844
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19317262190101792 1693.737548828125 

27 mlp.up_proj
Factorizing 27.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 32.16344451904297 197.95938110351562
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18743828804262222 2975.0205078125 

27 mlp.gate_proj
Factorizing 27.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 30.947853088378906 195.7030029296875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17624594460070972 3203.4462890625 

27 mlp.down_proj
Factorizing 27.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 29.299551010131836 192.77886962890625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18949542782170986 2983.416015625 

collecting Q input activations for next block ...
collecting FP target output activations for block 28...
28 self_attn.q_proj
err before 0.12012443950516172
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1a908f50>
epoch: 0, train err: 0.1186994120653253
epoch: 1, train err: 0.11692641858826391
epoch: 2, train err: 0.11618256324436516
epoch: 3, train err: 0.11592895307694562
Factorizing 28.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.792862892150879 36.5742301940918
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19578113253154453 1578.779052734375 

28 self_attn.v_proj
Factorizing 28.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 73.97911071777344 443.4502868652344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.72it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19462979224420363 1351.50927734375 

28 self_attn.o_proj
Factorizing 28.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 7.630297660827637 56.12562561035156
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.21825233282044876 1501.5760498046875 

28 self_attn.k_proj
err before 0.11795293659088202
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d47ba490>
epoch: 0, train err: 0.11744310345966369
epoch: 1, train err: 0.11641149257775396
epoch: 2, train err: 0.11593123900820501
epoch: 3, train err: 0.11576165651786141
Factorizing 28.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 2.3133766651153564 77.77322387695312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20415607718534248 1685.5125732421875 

28 mlp.up_proj
Factorizing 28.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 30.92933464050293 206.88436889648438
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19204584757486978 3097.3154296875 

28 mlp.gate_proj
Factorizing 28.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.153907775878906 193.92691040039062
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17671056165762827 3189.272216796875 

28 mlp.down_proj
Factorizing 28.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 29.594318389892578 204.03973388671875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19640451454254518 3129.90234375 

collecting Q input activations for next block ...
collecting FP target output activations for block 29...
29 self_attn.q_proj
err before 0.12033238855656236
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1adb7ed0>
epoch: 0, train err: 0.11878647599951364
epoch: 1, train err: 0.11696833570022136
epoch: 2, train err: 0.11622153778444044
epoch: 3, train err: 0.11596257064957172
Factorizing 29.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.3536993265151978 25.0662784576416
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17870961273065672 1366.7711181640625 

29 self_attn.v_proj
Factorizing 29.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 52.216835021972656 307.77203369140625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19147447908305687 1341.8531494140625 

29 self_attn.o_proj
Factorizing 29.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 6.9624433517456055 44.39201354980469
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.2079236464066939 1463.782470703125 

29 self_attn.k_proj
err before 0.1176728758146055
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1a92e810>
epoch: 0, train err: 0.11720838749897666
epoch: 1, train err: 0.11621366458712146
epoch: 2, train err: 0.11573751055402681
epoch: 3, train err: 0.1155664328543935
Factorizing 29.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.5489823818206787 49.669288635253906
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17983031663738314 1404.1151123046875 

29 mlp.up_proj
Factorizing 29.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 30.918415069580078 244.99020385742188
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1977747678756714 3240.341796875 

29 mlp.gate_proj
Factorizing 29.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 29.243122100830078 268.422607421875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18153992781402373 3276.4326171875 

29 mlp.down_proj
Factorizing 29.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 30.555850982666016 221.89675903320312
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20429270935058594 3268.683349609375 

collecting Q input activations for next block ...
collecting FP target output activations for block 30...
30 self_attn.q_proj
err before 0.13215739361476153
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f3a1a941d10>
epoch: 0, train err: 0.1291513897886034
epoch: 1, train err: 0.1266491604037583
epoch: 2, train err: 0.12576007208554074
epoch: 3, train err: 0.12545226869406179
Factorizing 30.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 1.4004650115966797 20.428287506103516
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1866676866515609 1445.5545654296875 

30 self_attn.v_proj
Factorizing 30.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 60.760765075683594 353.955078125
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19653142846148947 1446.4713134765625 

30 self_attn.o_proj
Factorizing 30.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 6.892045021057129 49.67894744873047
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.22236202503072805 1650.815673828125 

30 self_attn.k_proj
err before 0.12749940864159726
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f36a4309b50>
epoch: 0, train err: 0.12673775508301333
epoch: 1, train err: 0.12557193901739083
epoch: 2, train err: 0.12504459041520022
epoch: 3, train err: 0.12484356394270435
Factorizing 30.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.69it/s]err 259 1.7125189304351807 32.36606979370117
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.69it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.1885817194559488 1502.619140625 

30 mlp.up_proj
Factorizing 30.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 33.736305236816406 345.7367858886719
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2024048826748267 3445.74072265625 

30 mlp.gate_proj
Factorizing 30.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 30.550312042236328 270.086669921875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17997976251550624 3409.53662109375 

30 mlp.down_proj
Factorizing 30.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 36.94477462768555 944.4424438476562
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.217705514266906 3441.48876953125 

collecting Q input activations for next block ...
collecting FP target output activations for block 31...
31 self_attn.q_proj
err before 0.33055551687721163
tuning weights:  256 dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d470e650>
epoch: 0, train err: 0.27458371681859717
epoch: 1, train err: 0.25889589387224987
epoch: 2, train err: 0.25588237430201843
epoch: 3, train err: 0.25493194750742987
Factorizing 31.self_attn.q_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 0.972322940826416 16.579307556152344
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17570621736588016 1394.404541015625 

31 self_attn.v_proj
Factorizing 31.self_attn.v_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.72it/s]err 259 19.859148025512695 123.3554916381836
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.19564365966157762 1195.7740478515625 

31 self_attn.o_proj
Factorizing 31.self_attn.o_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.70it/s]err 259 5.745732307434082 51.1383056640625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.70it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.20722581912309695 1293.089111328125 

31 self_attn.k_proj
err before 0.26305310224415734
tuning weights:  256 dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']) [torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16]
scheduler <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f35d66b0790>
epoch: 0, train err: 0.2574213673360646
epoch: 1, train err: 0.2536106475163251
epoch: 2, train err: 0.25203540187794715
epoch: 3, train err: 0.2514573757071048
Factorizing 31.self_attn.k_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [00:33<00:00,  7.71it/s]err 259 0.9970884323120117 19.622892379760742
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:33<00:00,  7.71it/s]
sparsity check:  1.5 bpw
relative err after factorization:  0.17420147773914768 1482.802978515625 

31 mlp.up_proj
Factorizing 31.mlp.up_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 38.43516540527344 615.4203491210938
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2103593349456787 3877.34326171875 

31 mlp.gate_proj
Factorizing 31.mlp.gate_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.88it/s]err 259 32.12237548828125 294.796875
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.88it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18335719138198758 3778.625 

31 mlp.down_proj
Factorizing 31.mlp.down_proj...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 259/260 [01:29<00:00,  2.89it/s]err 259 64.92613220214844 1436.00390625
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [01:30<00:00,  2.89it/s]
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.23381255609303833 3696.10888671875 

total time 15929.061827659607
saving model...
Model's save dict has been saved!
Testing PPL...

Start evaluation... 83 339968
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:49<00:00,  1.32s/it]
Got PPL 10.50129935240124
